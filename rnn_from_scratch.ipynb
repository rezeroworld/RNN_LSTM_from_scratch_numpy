{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Type\n",
    "from numpy.linalg import norm\n",
    "\n",
    "#########################################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_logs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "def _read_words(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "      return [\"<bos>\"] + f.read().replace(\"\\n\", \"<eos> <bos>\").split()[:-1]\n",
    "\n",
    "def _build_vocab(filename):\n",
    "    data = _read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    id_to_word = dict((v, k) for k, v in word_to_id.items())\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "    data = _read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "# Processes the raw data from text files\n",
    "def ptb_raw_data(data_path=None, prefix=\"ptb\"):\n",
    "    train_path = os.path.join(data_path, prefix + \".train.txt\")\n",
    "    valid_path = os.path.join(data_path, prefix + \".valid.txt\")\n",
    "    test_path = os.path.join(data_path, prefix + \".test.txt\")\n",
    "\n",
    "    word_to_id, id_2_word = _build_vocab(train_path)\n",
    "    train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "    return train_data, valid_data, test_data, word_to_id, id_2_word\n",
    "\n",
    "# Yields minibatches of data\n",
    "def ptb_iterator(raw_data, batch_size, num_steps):\n",
    "    raw_data = np.array(raw_data, dtype=np.int32)\n",
    "\n",
    "    data_len = len(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    print(f'training data shape is {data_len} and the number of batches is {batch_len}')\n",
    "    data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "    if epoch_size == 0:\n",
    "        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "        y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From previous project\n",
    "class Param(ABC):\n",
    "    def __init__(self, is_trainable : bool = True, initialization_method : str = \"random\"):\n",
    "        self.is_trainable = is_trainable\n",
    "        self.initialization_method = initialization_method\n",
    "        self.param = None\n",
    "        self.dparam = None\n",
    "        self.dparam_1 = None\n",
    "\n",
    "    def __str__(self,):\n",
    "        return self.param\n",
    "\n",
    "    @abstractmethod\n",
    "    def initialize_parameters(self, ):\n",
    "        pass\n",
    "\n",
    "class ScalarParam(Param):\n",
    "    def __init__(self, trainable : bool = True, initialization_method : str = \"normal\"):\n",
    "        super().__init__(trainable, initialization_method)\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self,):\n",
    "        if self.initialization_method==\"normal\":\n",
    "            self.param = np.random.random()\n",
    "        elif self.initialization_method==\"zero\":\n",
    "            self.param = 0.0\n",
    "        elif self.initialization_method==\"one\":\n",
    "            self.param = 1.0\n",
    "\n",
    "        self.dparam = 0\n",
    "        self.dparam_1 = None\n",
    "\n",
    "class ArrayParam(Param):\n",
    "    def __init__(self, shape: tuple, is_trainable : bool = True, initialization_method : str = \"normal\", **kwargs):\n",
    "        super().__init__(is_trainable, initialization_method)\n",
    "        self.shape = shape\n",
    "        self.loc = kwargs['loc'] if 'loc' in kwargs else 0.\n",
    "        self.scale = kwargs['scale'] if 'scale' in kwargs else 1.\n",
    "        self.low = kwargs['low'] if 'low' in kwargs else -0.1\n",
    "        self.high = kwargs['high'] if 'high' in kwargs else 0.1\n",
    "        self.initialize_parameters(self.loc, self.scale, self.low, self.high)\n",
    "\n",
    "    #@jit(cache=True)\n",
    "    def initialize_parameters(self, loc, scale, low, high):\n",
    "        assert len(self.shape) >= 1, \"shape must be a tuple of at least one element.\"\n",
    "\n",
    "        if self.initialization_method==\"normal\":\n",
    "            self.param = np.random.normal(loc=loc, scale=scale, size=self.shape)\n",
    "        if self.initialization_method==\"uniform\":\n",
    "            self.param = np.random.uniform(low=low, high=high, size=self.shape)\n",
    "        elif self.initialization_method==\"zero\":\n",
    "            self.param = np.zeros(shape=self.shape)\n",
    "        elif self.initialization_method==\"one\":\n",
    "            self.param = np.ones(shape=self.shape)\n",
    "        elif self.initialization_method=='glorot':    \n",
    "            a = np.sqrt(6.0 / np.sum(self.shape))\n",
    "            self.param = np.random.uniform(-a, a, self.shape)\n",
    "\n",
    "        self.dparam = np.zeros_like(self.param)\n",
    "        self.dparam_1 = None\n",
    "\n",
    "class NN(ABC):\n",
    "    def __init__(self):\n",
    "        self.params: dict[str, ArrayParam] = {}\n",
    "        self.forward_dict: dict[str, type] = {}\n",
    "        self.params = {}\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self,):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self,):\n",
    "        pass\n",
    "\n",
    "    def __str__(self,):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    #@jit(cache=True)\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "class Linear(NN):\n",
    "    ##@personal_log\n",
    "    def __init__(self, dim_in, dim_out, weights_initialization_method='normal', bias: bool=True, bias_initialization_method='zeros'):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.bias = bias\n",
    "        self.weights_initialization_method = weights_initialization_method\n",
    "        self.bias_initialization_method = bias_initialization_method\n",
    "\n",
    "        self.params : dict[str, ArrayParam] = {}\n",
    "        self.params['w'] = ArrayParam(shape=(dim_in, dim_out), initialization_method=weights_initialization_method)\n",
    "        if self.bias: self.params['b'] = ArrayParam(shape=(1, dim_out), initialization_method=bias_initialization_method)\n",
    "\n",
    "    ##@personal_log\n",
    "    #@jit(cache=True)\n",
    "    def forward(self, x, no_grad=False):\n",
    "        if self.bias: a = x @ self.params['w'].param + self.params['b'].param\n",
    "        else: a = x @ self.params['w'].param\n",
    "        if not no_grad: \n",
    "            self.forward_dict = {\n",
    "                'x': x,\n",
    "                'a': a,\n",
    "            }\n",
    "        else:\n",
    "            self.forward_dict = {\n",
    "                'x': np.zeros_like(x),\n",
    "                'a': np.zeros_like(a),\n",
    "            }\n",
    "        return a\n",
    "    \n",
    "    #@personal_log\n",
    "    #@jit(cache=True)\n",
    "    def backward(self, da):\n",
    "        dw = self.forward_dict['x'].T @ da\n",
    "        dx = da @ self.params['w'].param.T\n",
    "\n",
    "        self.params['w'].dparam = dw\n",
    "        #print(f\"norm of weight of linear is {norm(dw):.4f}\")\n",
    "\n",
    "        if self.bias: \n",
    "            db = np.sum(da, axis=0)[np.newaxis, :]\n",
    "            self.params['b'].dparam = db\n",
    "        return dx\n",
    "    \n",
    "class Criterion(ABC):\n",
    "    def __init__(self):\n",
    "        self.forward_dict: dict[str, type] = {}\n",
    "        self.params : dict[str, Type(Param)] = {}\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self,):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self,):\n",
    "        pass\n",
    "\n",
    "    def __str__(self,):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    #@jit(cache=True)\n",
    "    def __call__(self, x, y, no_grad=False):\n",
    "        return self.forward(x, y, no_grad)\n",
    "    \n",
    "class CrossEntropy(Criterion):\n",
    "    #@personal_log\n",
    "    def __init__(self, epsilon=1e-20,):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    #@jit(cache=True)\n",
    "    def softmax(self, x):\n",
    "        f = x - np.max(x, axis=1)[:,np.newaxis]\n",
    "        p = np.exp(f) / np.sum(np.exp(f), axis=1)[:,np.newaxis]\n",
    "        return p\n",
    "\n",
    "    #@personal_log\n",
    "    #@jit(cache=True)\n",
    "    def forward(self, x, y, no_grad=False):\n",
    "        m = y.shape[0]\n",
    "        p = self.softmax(x)\n",
    "        p[np.where(p < self.epsilon)] = self.epsilon\n",
    "        p[np.where(p > 1 - self.epsilon)] = 1 - self.epsilon\n",
    "        loss = - np.sum(y * np.log(p)) / m\n",
    "        if not no_grad: \n",
    "            self.forward_dict = {\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'p': p,\n",
    "                'loss': loss,\n",
    "            }\n",
    "        return p, loss\n",
    "    \n",
    "    #@personal_log\n",
    "    #@jit(cache=True)\n",
    "    def backward(self, out):\n",
    "        m = self.forward_dict['y'].shape[0]\n",
    "        grad = out - self.forward_dict['y']\n",
    "        grad = grad/m # to take the mean across the batch\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation():\n",
    "    @staticmethod\n",
    "    def relu(x, grad=False):\n",
    "        if grad:\n",
    "            return (x > 0) * 1\n",
    "        else:\n",
    "            return np.maximum(0, x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(x, grad=False):\n",
    "        if grad:\n",
    "            return (1 / (1 + np.exp(-x)))*(1 - (1 / (1 + np.exp(-x))))\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x, grad=False):\n",
    "        if grad:\n",
    "            return 1 - np.tanh(x)**2\n",
    "        return np.tanh(x)      \n",
    "    \n",
    "class Optimizer():\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "\n",
    "    def __str__(self,):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, ):\n",
    "        pass\n",
    "\n",
    "    #@jit(cache=True)\n",
    "    #@personal_log\n",
    "    def zero_grad(self, ):\n",
    "        # if to be rewritten, redirect to a function directly inside the class\n",
    "        for layer in self.model.layers:\n",
    "            for param in layer.params:\n",
    "                layer.params[param].dparam = np.zeros_like(layer.params[param].param)\n",
    "\n",
    "class SGDOptimizer(Optimizer):\n",
    "    #@personal_log\n",
    "    def __init__(self, model, lr, mu_momentum=0., tau_dampening=0., nesterov=False, weight_decay=0.,):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.mu_momentum = mu_momentum\n",
    "        self.model = model\n",
    "        self.tau_dampening = tau_dampening\n",
    "        self.nesterov = nesterov\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    #@personal_log\n",
    "    #@jit(cache=True)\n",
    "    def step(self, ):\n",
    "        for layer in self.model.layers:\n",
    "            for param in layer.params:\n",
    "                if layer.params[param].is_trainable:\n",
    "                    grad_t = layer.params[param].dparam\n",
    "                    if self.weight_decay != 0.:\n",
    "                        grad_t += self.weight_decay * layer.params[param].param\n",
    "                    if self.mu_momentum != 0.:\n",
    "                        bt_1 = layer.params[param].dparam_1\n",
    "                        if bt_1 is not None:\n",
    "                            bt = self.mu_momentum * bt_1 + (1 - self.tau_dampening) * grad_t\n",
    "                        else:\n",
    "                            bt = grad_t\n",
    "\n",
    "                        if self.nesterov:\n",
    "                            grad_t += self.mu_momentum * bt\n",
    "                        else:\n",
    "                            grad_t = bt\n",
    "\n",
    "                    layer.params[param].param -= grad_t * self.lr\n",
    "\n",
    "                    # post update\n",
    "                    layer.params[param].dparam_1 = layer.params[param].dparam\n",
    "\n",
    "class AdamOptimizer(Optimizer):\n",
    "    def __init__(self, model, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 1.\n",
    "\n",
    "        self.params = self.get_model_parameters(self.model.layers)\n",
    "        self.m_dw, self.v_dw = self.init_moments()\n",
    "\n",
    "    def get_model_parameters(self, layers):\n",
    "        if isinstance(layers, list):\n",
    "            #print(f\"list {layers}\")\n",
    "            elt = layers.pop(0)\n",
    "            if len(layers) > 0:\n",
    "                return self.get_model_parameters(elt) + self.get_model_parameters(layers)\n",
    "            else:\n",
    "                return self.get_model_parameters(elt)\n",
    "        elif isinstance(layers, NN):\n",
    "            #print(f\"NN {layers}\")\n",
    "            if layers.params:\n",
    "                return self.get_model_parameters(layers.params)\n",
    "            elif layers.layers:\n",
    "                return self.get_model_parameters(layers.layers)\n",
    "            else:\n",
    "                raise Exception('error type 2 ', layers.__class__.__name__)\n",
    "        elif isinstance(layers, Criterion):\n",
    "            return []\n",
    "        elif isinstance(layers, dict):\n",
    "            #print(f\"dict {layers}\")\n",
    "            return list(layers.values())\n",
    "        else:\n",
    "            raise Exception('error type 1 ', layers.__class__.__name__)\n",
    "\n",
    "    def init_moments(self, ):\n",
    "        m_dw, v_dw = [], []\n",
    "\n",
    "        for param in self.params:\n",
    "            m_dw.append(np.zeros_like(param))\n",
    "            v_dw.append(np.zeros_like(param))\n",
    "\n",
    "        return m_dw, v_dw\n",
    "\n",
    "    def step(self, ):\n",
    "        for i, param in enumerate(self.params):\n",
    "            if param.is_trainable:\n",
    "                ## momentum beta 1\n",
    "                # *** weights *** #\n",
    "                self.m_dw[i] = self.beta1 * self.m_dw[i] + (1-self.beta1) * param.dparam\n",
    "\n",
    "                ## rms beta 2\n",
    "                # *** weights *** #\n",
    "                self.v_dw[i] = self.beta2 * self.v_dw[i] + (1-self.beta2) * (param.dparam**2)\n",
    "\n",
    "                ## bias correction\n",
    "                m_dw_corr = self.m_dw[i]/(1-self.beta1**self.t)\n",
    "                v_dw_corr = self.v_dw[i]/(1-self.beta2**self.t)\n",
    "\n",
    "                ## update weights and biases\n",
    "                param.param -=  self.lr * (m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon))\n",
    "        self.t += 1\n",
    "\n",
    "    def zero_grad(self, ):\n",
    "        # if to be rewritten, redirect to a function directly inside the class\n",
    "        for param in self.params:\n",
    "            if param.is_trainable:\n",
    "                param.dparam = np.zeros_like(param.param)\n",
    "\n",
    "    def gradient_array(self, ):\n",
    "        return np.concatenate([param.dparam.ravel() for param in self.params])\n",
    "\n",
    "    def clip_gradient(self, max_threshold):\n",
    "        gradient_arr = self.gradient_array()\n",
    "        norm_ = np.linalg.norm(gradient_arr)\n",
    "        if norm_ >= max_threshold:\n",
    "            for param in self.params:\n",
    "                param.dparam = max_threshold * param.dparam / norm_\n",
    "\n",
    "    def clip_gradient_v2(self, max_threshold):\n",
    "        for param in self.params:\n",
    "            norm_ = np.linalg.norm(param.dparam)\n",
    "            if norm_ >= max_threshold:\n",
    "                param.dparam = max_threshold * param.dparam / norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot(x, dim):\n",
    "    return np.eye(dim)[x]\n",
    "\n",
    "class Embedding(NN):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "\n",
    "        self.params : dict[str, ArrayParam] = {}\n",
    "        self.params['w'] = ArrayParam(shape=(dim_in, dim_out), initialization_method='uniform', low=-0.1, high=0.1)\n",
    "\n",
    "    def forward(self, x, no_grad=False):\n",
    "        cx = x @ self.params['w'].param\n",
    "        if not no_grad:\n",
    "            self.forward_dict = {\n",
    "                'x': x,\n",
    "                'cx': cx,\n",
    "            }\n",
    "        return cx\n",
    "\n",
    "    def backward(self, dcx):\n",
    "        dw = self.forward_dict['x'].T @ dcx\n",
    "        self.params['w'].dparam = dw\n",
    "        #print(f\"norm of weight of embedding is {norm(dw):.4f}\")\n",
    "        return dcx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(NN):\n",
    "    def __init__(self, dim_in, dim_hidden, nonlinearity=Activation.tanh, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        #self.num_layers = num_layers\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.bias = bias\n",
    "        self.params: dict[str, ArrayParam] = {}\n",
    "        b = np.sqrt(1 / self.dim_hidden)\n",
    "        #self.params['w'] = ArrayParam(shape=(dim_in, dim_hidden), initialization_method='uniform', low=-b, high=b)\n",
    "        #self.params['u'] = ArrayParam(shape=(dim_hidden, dim_hidden), initialization_method='uniform', low=-b, high=b)\n",
    "        self.params['w'] = ArrayParam(shape=(dim_in + dim_hidden, dim_hidden), initialization_method='uniform', low=-b, high=b)\n",
    "        if self.bias:\n",
    "            self.params['b'] = ArrayParam(shape=(1, dim_hidden), initialization_method='uniform', low=-b, high=b)\n",
    "\n",
    "    #def forward(self, cx, h_1, no_grad=False):\n",
    "    def forward(self, combined, no_grad=False):\n",
    "        #a = cx @ self.params['w'].param + h_1 @ self.params['u'].param\n",
    "        a = combined @ self.params['w'].param\n",
    "        if self.bias:\n",
    "            a += self.params['b'].param\n",
    "        h = self.nonlinearity(a, grad=False)\n",
    "        if not no_grad:\n",
    "            self.forward_dict = {\n",
    "                'combined': combined,\n",
    "                'a': a,\n",
    "                'h': h,\n",
    "            }\n",
    "        return h\n",
    "\n",
    "    #def backward(self, dltdh, dldh):\n",
    "    def backward(self, dltdh, dldh):\n",
    "        dldh += dltdh\n",
    "        da = (1 - self.forward_dict['h'] ** 2) * dldh\n",
    "        #dcx = da @ self.params['w'].param.T\n",
    "        #dldh_1 = da @ self.params['u'].param.T\n",
    "        dcombined = da @ self.params['w'].param.T\n",
    "\n",
    "        #self.params['w'].dparam += self.forward_dict['cx'].T @ da\n",
    "        #self.params['u'].dparam += self.forward_dict['h_1'].T @ da\n",
    "        self.params['w'].dparam += self.forward_dict['combined'].T @ da\n",
    "        if self.bias:\n",
    "            self.params['b'].dparam += np.sum(da, axis=0)[np.newaxis, :]\n",
    "        #return dldh_1, dcx\n",
    "        dcx, dldh_1 = dcombined[:,:self.dim_in], dcombined[:,self.dim_in:] \n",
    "        #print(f\"dcx shape is {dcx.shape}, and dldh_1 shape {dldh_1.shape}\")\n",
    "        return dcx, dldh_1\n",
    "\n",
    "class RNNModel():\n",
    "    def __init__(self, vocab_size, context_size=16, embed_size=128, hidden_size=256, bias=True):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.bias = bias\n",
    "        self.context_size = context_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.forward_dicts = []\n",
    "\n",
    "        self.embedding: Embedding = Embedding(vocab_size, self.embed_size)\n",
    "        self.rnn: RNN = RNN(self.embed_size, self.hidden_size, bias=self.bias)\n",
    "        self.linear: Linear = Linear(self.hidden_size, self.vocab_size, bias=self.bias, weights_initialization_method='uniform', bias_initialization_method='uniform')\n",
    "        self.cross_entropy: CrossEntropy = CrossEntropy()\n",
    "\n",
    "        self.layers = [\n",
    "            self.embedding,\n",
    "            self.rnn,\n",
    "            self.linear,\n",
    "            self.cross_entropy,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x, y, no_grad=False):\n",
    "        return self.forward(x, y, no_grad)\n",
    "\n",
    "    def forward(self, x, y, no_grad=False):\n",
    "        T, B, _ = x.shape\n",
    "        probas = np.zeros((T, B, self.vocab_size))\n",
    "        hidden = np.zeros((B, self.hidden_size))\n",
    "        losses = np.zeros((T,))\n",
    "        for timestamp in range(self.context_size):\n",
    "            cxt = self.embedding(x[timestamp], no_grad)\n",
    "            #hidden = self.rnn(cxt, hidden, no_grad)\n",
    "            #print(f\"combined shape is {np.vstack((cxt, hidden)).shape}\")\n",
    "            hidden = self.rnn(np.hstack((cxt, hidden)), no_grad)\n",
    "            logits = self.linear(hidden, no_grad)\n",
    "            probas[timestamp], losses[timestamp] = self.cross_entropy(logits, y[timestamp], no_grad)\n",
    "            if not no_grad:\n",
    "                self.forward_dicts.append((self.embedding.forward_dict, \n",
    "                                        self.rnn.forward_dict, \n",
    "                                        self.linear.forward_dict, \n",
    "                                        self.cross_entropy.forward_dict))\n",
    "        return probas, losses\n",
    "    \n",
    "    def backward(self, out):\n",
    "        T, B, C = out.shape\n",
    "        dldh = np.zeros(shape=(B, self.hidden_size))\n",
    "        for timestamp in reversed(range(self.context_size)):\n",
    "            self.embedding.forward_dict, self.rnn.forward_dict, self.linear.forward_dict, self.cross_entropy.forward_dict = self.forward_dicts.pop()\n",
    "            out = self.cross_entropy.backward(out[timestamp])\n",
    "            dltdh = self.linear.backward(out)\n",
    "            dcxt, dldh = self.rnn.backward(dltdh, dldh)\n",
    "            _ = self.embedding.backward(dcxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and preparing data\n",
    "train_data, valid_data, test_data, word_to_id, id_2_word = ptb_raw_data(data_path=r'C:\\Users\\zarou\\Documents\\GitHub_Projects\\RNN_LSTM_from_scratch_numpy\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "context_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size=len(word_to_id), context_size=context_size, embed_size=128, hidden_size=128, bias=True) # 10001 token\n",
    "#optimizer = SGDOptimizer(model=model, lr=0.01, mu_momentum=0., nesterov=False, weight_decay=0.)\n",
    "optimizer = AdamOptimizer(model=model, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape is 971657 and the number of batches is 7591\n",
      "gradient before 0.04945506036477639\n",
      "gradient after 0.029059771546244843\n",
      "step: 0 and loss: 9.215868326213137\n",
      "gradient before 0.044345489095827514\n",
      "gradient after 0.027518467041726537\n",
      "step: 1 and loss: 9.208911193524955\n",
      "gradient before 0.04252131877984088\n",
      "gradient after 0.029001373336545445\n",
      "step: 2 and loss: 9.204193609989513\n",
      "gradient before 0.04316562371465199\n",
      "gradient after 0.02357117648278669\n",
      "step: 3 and loss: 9.19696845843492\n",
      "gradient before 0.042777915355527565\n",
      "gradient after 0.023188137742257014\n",
      "step: 4 and loss: 9.188605745355412\n",
      "gradient before 0.05075395206377643\n",
      "gradient after 0.025769916424339727\n",
      "step: 5 and loss: 9.177690474968044\n",
      "gradient before 0.04908700669741967\n",
      "gradient after 0.02341178748537361\n",
      "step: 6 and loss: 9.167860273058961\n",
      "gradient before 0.04347566469133623\n",
      "gradient after 0.022959583633121096\n",
      "step: 7 and loss: 9.17369121849586\n",
      "gradient before 0.04484555305186524\n",
      "gradient after 0.019500911028370052\n",
      "step: 8 and loss: 9.155358008015492\n",
      "gradient before 0.06327257839601119\n",
      "gradient after 0.02657103566477855\n",
      "step: 9 and loss: 9.14057778769552\n",
      "gradient before 0.054916463766890966\n",
      "gradient after 0.0245167057721365\n",
      "step: 10 and loss: 9.139346729237948\n",
      "gradient before 0.048279767343587666\n",
      "gradient after 0.017889627832987337\n",
      "step: 11 and loss: 9.127394497990663\n",
      "gradient before 0.06174265330982641\n",
      "gradient after 0.024623507812322653\n",
      "step: 12 and loss: 9.128939290080423\n",
      "gradient before 0.06378236944197989\n",
      "gradient after 0.01865526617739822\n",
      "step: 13 and loss: 9.092805774126006\n",
      "gradient before 0.06116784668788439\n",
      "gradient after 0.019042335944242875\n",
      "step: 14 and loss: 9.093891409621149\n",
      "gradient before 0.053446999234526366\n",
      "gradient after 0.013346927014255716\n",
      "step: 15 and loss: 9.059183742971085\n",
      "gradient before 0.06406959277642912\n",
      "gradient after 0.014488294920708881\n",
      "step: 16 and loss: 9.045817950373866\n",
      "gradient before 0.06862319505018284\n",
      "gradient after 0.015159013132250482\n",
      "step: 17 and loss: 9.019290707645407\n",
      "gradient before 0.08071556242555046\n",
      "gradient after 0.016684835201538368\n",
      "step: 18 and loss: 9.001261596376127\n",
      "gradient before 0.0844503326223318\n",
      "gradient after 0.018287488682791505\n",
      "step: 19 and loss: 9.000749521330073\n",
      "gradient before 0.07672215198178631\n",
      "gradient after 0.013743478278720432\n",
      "step: 20 and loss: 8.94208843180007\n",
      "gradient before 0.11183706267251688\n",
      "gradient after 0.019262323082182613\n",
      "step: 21 and loss: 8.907625387120412\n",
      "gradient before 0.09835156996046426\n",
      "gradient after 0.01645795048610438\n",
      "step: 22 and loss: 8.888344246915377\n",
      "gradient before 0.09185178675601438\n",
      "gradient after 0.01617583597971672\n",
      "step: 23 and loss: 8.904570456195092\n",
      "gradient before 0.09671528098432364\n",
      "gradient after 0.01635430649994003\n",
      "step: 24 and loss: 8.846271856522398\n",
      "gradient before 0.1417419146166835\n",
      "gradient after 0.02213250763682646\n",
      "step: 25 and loss: 8.801180968253346\n",
      "gradient before 0.14689358398515914\n",
      "gradient after 0.023850599632157812\n",
      "step: 26 and loss: 8.753805993849916\n",
      "gradient before 0.13243172965777114\n",
      "gradient after 0.02153850016282232\n",
      "step: 27 and loss: 8.735957711248448\n",
      "gradient before 0.12652979068026332\n",
      "gradient after 0.022191644629879034\n",
      "step: 28 and loss: 8.734596700696168\n",
      "gradient before 0.12385875029799348\n",
      "gradient after 0.021381075914025237\n",
      "step: 29 and loss: 8.69960586215306\n",
      "gradient before 0.12945428770378067\n",
      "gradient after 0.024627821788514078\n",
      "step: 30 and loss: 8.700591524182528\n",
      "gradient before 0.14044145097802765\n",
      "gradient after 0.0242091507100931\n",
      "step: 31 and loss: 8.622860303805837\n",
      "gradient before 0.1644364877392506\n",
      "gradient after 0.02809602958227231\n",
      "step: 32 and loss: 8.524432113390242\n",
      "gradient before 0.15402570319440825\n",
      "gradient after 0.02948542101788801\n",
      "step: 33 and loss: 8.610862804472323\n",
      "gradient before 0.16645524428792294\n",
      "gradient after 0.030982038746870486\n",
      "step: 34 and loss: 8.51862920625927\n",
      "gradient before 0.1685784759005411\n",
      "gradient after 0.03062080079153846\n",
      "step: 35 and loss: 8.487010066079565\n",
      "gradient before 0.1655345786866718\n",
      "gradient after 0.031748568830767966\n",
      "step: 36 and loss: 8.526304142087316\n",
      "gradient before 0.1791508482049888\n",
      "gradient after 0.0332549041438871\n",
      "step: 37 and loss: 8.432751025725453\n",
      "gradient before 0.21957347979968442\n",
      "gradient after 0.04137071990994412\n",
      "step: 38 and loss: 8.432848158948662\n",
      "gradient before 0.16868498002335472\n",
      "gradient after 0.03297531998602179\n",
      "step: 39 and loss: 8.34776755887064\n",
      "gradient before 0.1806148478896856\n",
      "gradient after 0.03633837833007891\n",
      "step: 40 and loss: 8.335048371403769\n",
      "gradient before 0.22214196193853325\n",
      "gradient after 0.04393274424752722\n",
      "step: 41 and loss: 8.259814646800017\n",
      "gradient before 0.23035397080765477\n",
      "gradient after 0.04692151782219783\n",
      "step: 42 and loss: 8.211952298283148\n",
      "gradient before 0.22924211292857605\n",
      "gradient after 0.04638421148195581\n",
      "step: 43 and loss: 8.193517743942873\n",
      "gradient before 0.2189455549011933\n",
      "gradient after 0.043097057638353024\n",
      "step: 44 and loss: 8.13560519653513\n",
      "gradient before 0.24375669122517363\n",
      "gradient after 0.051505812397098866\n",
      "step: 45 and loss: 8.178503687609322\n",
      "gradient before 0.20582430499805407\n",
      "gradient after 0.043909711397478075\n",
      "step: 46 and loss: 8.141061914577\n",
      "gradient before 0.2754063102067417\n",
      "gradient after 0.05439644219538447\n",
      "step: 47 and loss: 8.038512209551618\n",
      "gradient before 0.22248663753511297\n",
      "gradient after 0.05568743539804021\n",
      "step: 48 and loss: 8.157452083150542\n",
      "gradient before 0.25266308389250075\n",
      "gradient after 0.052455667070405936\n",
      "step: 49 and loss: 7.943960250026659\n",
      "gradient before 0.29103332076424987\n",
      "gradient after 0.06046325772891576\n",
      "step: 50 and loss: 7.979629409075785\n",
      "gradient before 0.2949901508028863\n",
      "gradient after 0.05875063284674182\n",
      "step: 51 and loss: 7.808413132691714\n",
      "gradient before 0.2512944905408504\n",
      "gradient after 0.05840525702226887\n",
      "step: 52 and loss: 7.923154835911993\n",
      "gradient before 0.25804741382011126\n",
      "gradient after 0.062184335852073606\n",
      "step: 53 and loss: 7.871401820818033\n",
      "gradient before 0.39084432961275767\n",
      "gradient after 0.08399984089240113\n",
      "step: 54 and loss: 7.845857880610199\n",
      "gradient before 0.31893853207203965\n",
      "gradient after 0.07179813039990286\n",
      "step: 55 and loss: 7.720113822326175\n",
      "gradient before 0.34596853956460977\n",
      "gradient after 0.07755276801053318\n",
      "step: 56 and loss: 7.768760970527844\n",
      "gradient before 0.356215148582609\n",
      "gradient after 0.07747058119959999\n",
      "step: 57 and loss: 7.698586201250587\n",
      "gradient before 0.3182981075964656\n",
      "gradient after 0.06848567447609477\n",
      "step: 58 and loss: 7.730473256634433\n",
      "gradient before 0.33727897331253054\n",
      "gradient after 0.08144059794734276\n",
      "step: 59 and loss: 7.685293731785247\n",
      "gradient before 0.3262003385201523\n",
      "gradient after 0.07626381228390525\n",
      "step: 60 and loss: 7.677007728346194\n",
      "gradient before 0.38408672324631155\n",
      "gradient after 0.0897787518524944\n",
      "step: 61 and loss: 7.555466527955536\n",
      "gradient before 0.32003252085944034\n",
      "gradient after 0.07525035355089962\n",
      "step: 62 and loss: 7.747007218344642\n",
      "gradient before 0.35721422366119077\n",
      "gradient after 0.08187623260781303\n",
      "step: 63 and loss: 7.386558419567702\n",
      "gradient before 0.37734373361920576\n",
      "gradient after 0.08783198961392794\n",
      "step: 64 and loss: 7.449624208522457\n",
      "gradient before 0.38683628571334194\n",
      "gradient after 0.08338978943547157\n",
      "step: 65 and loss: 7.541328829470607\n",
      "gradient before 0.425294955094939\n",
      "gradient after 0.0916117177159887\n",
      "step: 66 and loss: 7.500177282811261\n",
      "gradient before 0.4191747137930805\n",
      "gradient after 0.09678054651424275\n",
      "step: 67 and loss: 7.524934838370389\n",
      "gradient before 0.45307006327567856\n",
      "gradient after 0.10189979649727035\n",
      "step: 68 and loss: 7.439544229836299\n",
      "gradient before 0.3936633252928952\n",
      "gradient after 0.09475782975075146\n",
      "step: 69 and loss: 7.397981298095296\n",
      "gradient before 0.4103276457500167\n",
      "gradient after 0.0972397314352262\n",
      "step: 70 and loss: 7.302574861746318\n",
      "gradient before 0.4014455246334039\n",
      "gradient after 0.09949248715226508\n",
      "step: 71 and loss: 7.623344962417149\n",
      "gradient before 0.42575367899938876\n",
      "gradient after 0.10227671566808667\n",
      "step: 72 and loss: 7.509841493043713\n",
      "gradient before 0.4436349315678352\n",
      "gradient after 0.09938480341076417\n",
      "step: 73 and loss: 7.155030698794241\n",
      "gradient before 0.4659484731082365\n",
      "gradient after 0.10428013177234034\n",
      "step: 74 and loss: 7.318478308173396\n",
      "gradient before 0.4833120227731026\n",
      "gradient after 0.10747375566598628\n",
      "step: 75 and loss: 7.51277434109145\n",
      "gradient before 0.4842454656104866\n",
      "gradient after 0.10481586579475236\n",
      "step: 76 and loss: 7.545139463850498\n",
      "gradient before 0.48915264708312806\n",
      "gradient after 0.10489212468374211\n",
      "step: 77 and loss: 7.637441697616178\n",
      "gradient before 0.4610898277978606\n",
      "gradient after 0.10581759648393513\n",
      "step: 78 and loss: 7.6637998818716\n",
      "gradient before 0.47080380755212164\n",
      "gradient after 0.10887840560227326\n",
      "step: 79 and loss: 7.747643976479134\n",
      "gradient before 0.4754370102793674\n",
      "gradient after 0.11263950440383572\n",
      "step: 80 and loss: 7.7763632047063656\n",
      "gradient before 0.527415946421601\n",
      "gradient after 0.10890307913476927\n",
      "step: 81 and loss: 7.717954964361747\n",
      "gradient before 0.5549534703303485\n",
      "gradient after 0.11435595275854159\n",
      "step: 82 and loss: 7.536554308226284\n",
      "gradient before 0.5244569545127208\n",
      "gradient after 0.11558674129774547\n",
      "step: 83 and loss: 7.705490036695546\n",
      "gradient before 0.5159054519007149\n",
      "gradient after 0.1207231110160102\n",
      "step: 84 and loss: 7.567175943580718\n",
      "gradient before 0.5159770860293256\n",
      "gradient after 0.11899002971346953\n",
      "step: 85 and loss: 7.762192746495655\n",
      "gradient before 0.6072376870378365\n",
      "gradient after 0.12266202463367297\n",
      "step: 86 and loss: 7.793632647425713\n",
      "gradient before 0.5000547536966969\n",
      "gradient after 0.11609711524956753\n",
      "step: 87 and loss: 7.928708302989896\n",
      "gradient before 0.497069178483575\n",
      "gradient after 0.12139637846531379\n",
      "step: 88 and loss: 8.039430725909547\n",
      "gradient before 0.5415841663837155\n",
      "gradient after 0.11964541638031122\n",
      "step: 89 and loss: 7.86402612936198\n",
      "gradient before 0.6056852030927246\n",
      "gradient after 0.12387019010028634\n",
      "step: 90 and loss: 7.835255672658057\n",
      "gradient before 0.5840610787641819\n",
      "gradient after 0.12390843089896406\n",
      "step: 91 and loss: 8.328872566741959\n",
      "gradient before 0.4984243298790379\n",
      "gradient after 0.11564137041926881\n",
      "step: 92 and loss: 8.17020188197943\n",
      "gradient before 0.5631076473793892\n",
      "gradient after 0.11977350386087414\n",
      "step: 93 and loss: 8.126721703547181\n",
      "gradient before 0.5821919415127917\n",
      "gradient after 0.12390766889327123\n",
      "step: 94 and loss: 8.133192519114077\n",
      "gradient before 0.5888345148433971\n",
      "gradient after 0.12736394269087878\n",
      "step: 95 and loss: 8.46146138326396\n",
      "gradient before 0.6472119412568852\n",
      "gradient after 0.1261542446127505\n",
      "step: 96 and loss: 8.193215337320815\n",
      "gradient before 0.5733558533514058\n",
      "gradient after 0.11903379727893737\n",
      "step: 97 and loss: 8.612102438693109\n",
      "gradient before 0.6546788124177999\n",
      "gradient after 0.13370732520582776\n",
      "step: 98 and loss: 8.531437559056371\n",
      "gradient before 0.5747332276850188\n",
      "gradient after 0.11934811821053176\n",
      "step: 99 and loss: 8.806082746500682\n",
      "gradient before 0.6007111856185505\n",
      "gradient after 0.12091991360893899\n",
      "step: 100 and loss: 8.332562264182444\n",
      "gradient before 0.6983160476939879\n",
      "gradient after 0.13445080667425569\n",
      "step: 101 and loss: 8.890646959241153\n",
      "gradient before 0.5608827848234438\n",
      "gradient after 0.12247536236425693\n",
      "step: 102 and loss: 8.967363502949604\n",
      "gradient before 0.6189452082543749\n",
      "gradient after 0.12588586213390834\n",
      "step: 103 and loss: 8.884005307779582\n",
      "gradient before 0.6062385006106101\n",
      "gradient after 0.12520574170548424\n",
      "step: 104 and loss: 8.827979378152632\n",
      "gradient before 0.6462894217608114\n",
      "gradient after 0.12887318929587308\n",
      "step: 105 and loss: 8.795447401230868\n",
      "gradient before 0.6873147893483352\n",
      "gradient after 0.13589456208504164\n",
      "step: 106 and loss: 9.23662088065927\n",
      "gradient before 0.8166102817798089\n",
      "gradient after 0.15306939970311984\n",
      "step: 107 and loss: 9.416909789202924\n",
      "gradient before 0.6649288805263704\n",
      "gradient after 0.12964791340613693\n",
      "step: 108 and loss: 9.061570516006125\n",
      "gradient before 0.6342960355544784\n",
      "gradient after 0.11463437176267831\n",
      "step: 109 and loss: 9.33843297981742\n",
      "gradient before 0.7112998096965856\n",
      "gradient after 0.1439309233746315\n",
      "step: 110 and loss: 9.37036387825885\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[883], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m step, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(ptb_iterator(train_data, batch_size, context_size)):\n\u001b[0;32m      3\u001b[0m     inputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(one_hot(x, \u001b[39mlen\u001b[39m(word_to_id)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m)) \u001b[39m# (B, T, C) transposed to (T, B, C)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     targets \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(one_hot(y, \u001b[39mlen\u001b[39;49m(word_to_id)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m)) \u001b[39m# (B, T, C) transposed to (T, B, C)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      7\u001b[0m     outputs, losses \u001b[39m=\u001b[39m model(inputs, targets, no_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[829], line 2\u001b[0m, in \u001b[0;36mone_hot\u001b[1;34m(x, dim)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mone_hot\u001b[39m(x, dim):\n\u001b[1;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49meye(dim)[x]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "for step, (x, y) in enumerate(ptb_iterator(train_data, batch_size, context_size)):\n",
    "    inputs = np.transpose(one_hot(x, len(word_to_id)), axes=(1,0,2)) # (B, T, C) transposed to (T, B, C)\n",
    "    targets = np.transpose(one_hot(y, len(word_to_id)), axes=(1,0,2)) # (B, T, C) transposed to (T, B, C)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs, losses = model(inputs, targets, no_grad=False)\n",
    "    model.backward(outputs)\n",
    "    print(f\"gradient before {np.linalg.norm(optimizer.params[0].dparam)}\")\n",
    "    optimizer.clip_gradient(0.25)\n",
    "    print(f\"gradient after {np.linalg.norm(optimizer.params[0].dparam)}\")\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 1 == 0:\n",
    "        print(f'step: {step} and loss: {np.mean(losses)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate(NN):\n",
    "    def __init__(self, dim_in, dim_hidden, nonlinearity=Activation.sigmoid, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim_in, self.dim_hidden, self.nonlinearity, self.bias = dim_in, dim_hidden, nonlinearity, bias\n",
    "        self.params: dict[str, ArrayParam] = {}\n",
    "        b = np.sqrt(1 / self.dim_hidden)\n",
    "        self.params['w'] = ArrayParam(shape=(dim_in, dim_hidden), initialization_method='uniform', low=-b, high=b)\n",
    "        self.params['u'] = ArrayParam(shape=(dim_hidden, dim_hidden), initialization_method='uniform', low=-b, high=b)\n",
    "        if self.bias:\n",
    "            self.params['b'] = ArrayParam(shape=(1, dim_hidden), initialization_method='uniform', low=-b, high=b)\n",
    "\n",
    "    def forward(self, cx, h_1, no_grad=False):\n",
    "        a = cx @ self.params['w'].param + h_1 @ self.params['w'].param\n",
    "        if self.bias:\n",
    "            a += self.params['b'].param\n",
    "        h = self.nonlinearity(a, grad=False)\n",
    "        if not no_grad:\n",
    "            self.forward_dict = {\n",
    "                'cx': cx,\n",
    "                'h_1': h_1,\n",
    "                'a': a,\n",
    "                'h': h,\n",
    "            }\n",
    "        return h\n",
    "\n",
    "    def backward(self, dh):\n",
    "        da = self.nonlinearity(self.forward_dict['h'], grad=True) * dh\n",
    "        dcx = da @ self.params['w'].param.T\n",
    "        dh_1 = da @ self.params['u'].param.T\n",
    "        self.params['w'].dparam += self.forward_dict['cx'].T @ da\n",
    "        self.params['u'].dparam += self.forward_dict['h_1'].T @ da\n",
    "        if self.bias:\n",
    "            self.params['b'].dparam += np.sum(da, axis=0)[np.newaxis, :]\n",
    "        return dcx, dh_1\n",
    "        \n",
    "\n",
    "class LSTM(NN):\n",
    "    def __init__(self, dim_in, dim_hidden, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        #self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.params: dict[str, ArrayParam] = {}\n",
    "        # input, forget, output gates\n",
    "        self.input_gate = Gate(self.dim_in, self.dim_hidden, nonlinearity=Activation.sigmoid, bias=self.bias)\n",
    "        self.forget_gate = Gate(self.dim_in, self.dim_hidden, nonlinearity=Activation.sigmoid, bias=self.bias)\n",
    "        self.output_gate = Gate(self.dim_in, self.dim_hidden, nonlinearity=Activation.sigmoid, bias=self.bias)\n",
    "        # cell state\n",
    "        self.cell_state = Gate(self.dim_in, self.dim_hidden, nonlinearity=Activation.tanh, bias=self.bias)\n",
    "\n",
    "        self.layers = [\n",
    "            self.input_gate,\n",
    "            self.forget_gate,\n",
    "            self.output_gate,\n",
    "            self.cell_state\n",
    "        ]\n",
    "\n",
    "    def forward(self, cx, ht_1, ct_1, no_grad=False):\n",
    "        it = self.input_gate(cx, ht_1)\n",
    "        ft = self.forget_gate(cx, ht_1)\n",
    "        ot = self.output_gate(cx, ht_1)\n",
    "        ct_tilde = self.cell_state(cx, ht_1)\n",
    "        ct = ft * ct_1 + it * ct_tilde\n",
    "        ht = ot * Activation.tanh(ct)\n",
    "\n",
    "        if not no_grad:\n",
    "            self.forward_dict = {\n",
    "                'cx': cx,\n",
    "                'ht_1': ht_1,\n",
    "                'ct_1': ct_1,\n",
    "                'it': it,\n",
    "                'ft': ft,\n",
    "                'ot': ot,\n",
    "                'ct_tilde': ct_tilde,\n",
    "                'ct': ct,\n",
    "                'ht': ht\n",
    "            }\n",
    "\n",
    "        return ht, ct\n",
    "\n",
    "    def backward(self, dht, dh_, dc_):\n",
    "        dh = dh_ + dht\n",
    "\n",
    "        dc = dc_ + dh * self.forward_dict['ot'] * (1 - self.forward_dict['ct']**2)\n",
    "        do = dh * Activation.tanh(self.forward_dict['ct'])\n",
    "        \n",
    "        di = dc * self.forward_dict['ct_tilde']\n",
    "        df = dc * self.forward_dict['ct_1']\n",
    "        dc_tilde = dc * self.forward_dict['it']\n",
    "\n",
    "        dcx_i, dh_1_i = self.input_gate.backward(di)\n",
    "        dcx_f, dh_1_f = self.forget_gate.backward(df)\n",
    "        dcx_o, dh_1_o = self.output_gate.backward(do)\n",
    "        dcx_c_tilde, dh_1_c_tilde = self.cell_state.backward(dc_tilde)\n",
    "\n",
    "        dcx = dcx_i + dcx_f + dcx_o + dcx_c_tilde\n",
    "        dh_1 = dh_1_i + dh_1_f + dh_1_o + dh_1_c_tilde\n",
    "        dc_1 = dc * self.forward_dict['ft']\n",
    "\n",
    "        return dcx, dh_1, dc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    def __init__(self, vocab_size, context_size=16, embed_size=128, hidden_size=256, bias=True):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.bias = bias\n",
    "        self.context_size = context_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.forward_dicts = []\n",
    "\n",
    "        self.embedding: Embedding = Embedding(vocab_size, self.embed_size)\n",
    "        self.rnn: LSTM = LSTM(self.embed_size, self.hidden_size, bias=self.bias)\n",
    "        self.linear: Linear = Linear(self.hidden_size, self.vocab_size, bias=self.bias, weights_initialization_method='uniform', bias_initialization_method='uniform')\n",
    "        self.cross_entropy: CrossEntropy = CrossEntropy()\n",
    "\n",
    "        self.layers = [\n",
    "            self.embedding,\n",
    "            self.rnn,\n",
    "            self.linear,\n",
    "            self.cross_entropy,\n",
    "        ]\n",
    "        \n",
    "        self.layers.extend(self.rnn.layers)\n",
    "\n",
    "    def __call__(self, x, y, no_grad=False):\n",
    "        return self.forward(x, y, no_grad)\n",
    "\n",
    "    def forward(self, x, y, no_grad=False):\n",
    "        T, B, _ = x.shape\n",
    "        probas = np.zeros((T, B, self.vocab_size))\n",
    "        hidden = np.zeros((B, self.hidden_size))\n",
    "        cell_state = np.ones((B, self.hidden_size))\n",
    "        losses = np.zeros((T,))\n",
    "        for timestamp in range(self.context_size):\n",
    "            cxt = self.embedding(x[timestamp], no_grad)\n",
    "            hidden, cell_state = self.rnn(cxt, hidden, cell_state, no_grad)\n",
    "            logits = self.linear(hidden, no_grad)\n",
    "            probas[timestamp], losses[timestamp] = self.cross_entropy(logits, y[timestamp], no_grad)\n",
    "            if not no_grad:\n",
    "                self.forward_dicts.append((self.embedding.forward_dict, \n",
    "                                        self.rnn.forward_dict, \n",
    "                                        self.linear.forward_dict, \n",
    "                                        self.cross_entropy.forward_dict,\n",
    "                                        self.rnn.input_gate.forward_dict, \n",
    "                                        self.rnn.output_gate.forward_dict, \n",
    "                                        self.rnn.forget_gate.forward_dict, \n",
    "                                        self.rnn.cell_state.forward_dict))                \n",
    "        return probas, losses\n",
    "    \n",
    "    def backward(self, out):\n",
    "        T, B, C = out.shape\n",
    "        dldh = np.zeros(shape=(B, self.hidden_size))\n",
    "        dldc = np.ones(shape=(B, self.hidden_size))\n",
    "        for timestamp in reversed(range(self.context_size)):\n",
    "            self.embedding.forward_dict, self.rnn.forward_dict, self.linear.forward_dict, self.cross_entropy.forward_dict, self.rnn.input_gate.forward_dict, self.rnn.output_gate.forward_dict, self.rnn.forget_gate.forward_dict, self.rnn.cell_state.forward_dict = self.forward_dicts.pop()\n",
    "            out = self.cross_entropy.backward(out[timestamp])\n",
    "            dltdh = self.linear.backward(out)\n",
    "            dcxt, dldh, dldc = self.rnn.backward(dltdh, dldh, dldc)\n",
    "            self.embedding.backward(dcxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "context_size = 4\n",
    "hidden_size = 128\n",
    "emb_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(vocab_size=len(word_to_id), context_size=context_size, embed_size=emb_size, hidden_size=hidden_size, bias=True) # 10001 token\n",
    "#optimizer = SGDOptimizer(model=model, lr=0.01, mu_momentum=0., nesterov=False, weight_decay=0.)\n",
    "optimizer = AdamOptimizer(model=model, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape is 971657 and the number of batches is 7591\n",
      "step: 0 and loss: 9.21472377620253\n",
      "step: 1 and loss: 9.208540134477602\n",
      "step: 2 and loss: 9.197862771020402\n",
      "step: 3 and loss: 9.202880478227758\n",
      "step: 4 and loss: 9.20105376981453\n",
      "step: 5 and loss: 9.18967482118309\n",
      "step: 6 and loss: 9.180327805947195\n",
      "step: 7 and loss: 9.171154616186618\n",
      "step: 8 and loss: 9.175932519362952\n",
      "step: 9 and loss: 9.164491626246624\n",
      "step: 10 and loss: 9.172525164460431\n",
      "step: 11 and loss: 9.162257772634003\n",
      "step: 12 and loss: 9.161896338318206\n",
      "step: 13 and loss: 9.159668820408685\n",
      "step: 14 and loss: 9.158038491156312\n",
      "step: 15 and loss: 9.143932184044226\n",
      "step: 16 and loss: 9.149290526476934\n",
      "step: 17 and loss: 9.13794304916937\n",
      "step: 18 and loss: 9.142540622149617\n",
      "step: 19 and loss: 9.136955625723951\n",
      "step: 20 and loss: 9.142086227552385\n",
      "step: 21 and loss: 9.124938675482193\n",
      "step: 22 and loss: 9.127532237644456\n",
      "step: 23 and loss: 9.111840898149081\n",
      "step: 24 and loss: 9.120658956856348\n",
      "step: 25 and loss: 9.103956997298507\n",
      "step: 26 and loss: 9.113422151617721\n",
      "step: 27 and loss: 9.114029333100387\n",
      "step: 28 and loss: 9.116781840810177\n",
      "step: 29 and loss: 9.115629794970257\n",
      "step: 30 and loss: 9.097051689871062\n",
      "step: 31 and loss: 9.093733310141664\n",
      "step: 32 and loss: 9.08366447665755\n",
      "step: 33 and loss: 9.092506690151982\n",
      "step: 34 and loss: 9.076688379295174\n",
      "step: 35 and loss: 9.064537630697973\n",
      "step: 36 and loss: 9.068257323257397\n",
      "step: 37 and loss: 9.05106612296585\n",
      "step: 38 and loss: 9.040740438561924\n",
      "step: 39 and loss: 9.04544226141073\n",
      "step: 40 and loss: 9.047084400103605\n",
      "step: 41 and loss: 9.022586781136273\n",
      "step: 42 and loss: 9.008618987110172\n",
      "step: 43 and loss: 8.999531159379002\n",
      "step: 44 and loss: 8.987875819775809\n",
      "step: 45 and loss: 8.981986763281167\n",
      "step: 46 and loss: 8.97639472418106\n",
      "step: 47 and loss: 8.93714362996073\n",
      "step: 48 and loss: 8.966278990126536\n",
      "step: 49 and loss: 8.93141291769059\n",
      "step: 50 and loss: 8.907768574853273\n",
      "step: 51 and loss: 8.898566448266868\n",
      "step: 52 and loss: 8.921028391467633\n",
      "step: 53 and loss: 8.893474773618177\n",
      "step: 54 and loss: 8.841938382489039\n",
      "step: 55 and loss: 8.813707926613374\n",
      "step: 56 and loss: 8.788109568477948\n",
      "step: 57 and loss: 8.767465511890649\n",
      "step: 58 and loss: 8.726688339836732\n",
      "step: 59 and loss: 8.696950001229286\n",
      "step: 60 and loss: 8.706596366543684\n",
      "step: 61 and loss: 8.6568096545531\n",
      "step: 62 and loss: 8.692990934676201\n",
      "step: 63 and loss: 8.55205736489592\n",
      "step: 64 and loss: 8.482973591809671\n",
      "step: 65 and loss: 8.505004155501005\n",
      "step: 66 and loss: 8.459799426420123\n",
      "step: 67 and loss: 8.446920866602666\n",
      "step: 68 and loss: 8.28775752576415\n",
      "step: 69 and loss: 8.30732947393029\n",
      "step: 70 and loss: 8.174478677765647\n",
      "step: 71 and loss: 8.189459091968791\n",
      "step: 72 and loss: 8.251313001416966\n",
      "step: 73 and loss: 7.993395697109598\n",
      "step: 74 and loss: 7.937701116247165\n",
      "step: 75 and loss: 8.034069999767578\n",
      "step: 76 and loss: 8.028644594025243\n",
      "step: 77 and loss: 8.019244393619607\n",
      "step: 78 and loss: 7.906435952164767\n",
      "step: 79 and loss: 7.978247892091594\n",
      "step: 80 and loss: 8.075495766324867\n",
      "step: 81 and loss: 7.87420401752105\n",
      "step: 82 and loss: 7.870402468745972\n",
      "step: 83 and loss: 7.870257520736924\n",
      "step: 84 and loss: 7.883756672876716\n",
      "step: 85 and loss: 7.8637336085267595\n",
      "step: 86 and loss: 7.871560872722879\n",
      "step: 87 and loss: 7.829485132327199\n",
      "step: 88 and loss: 7.931229040318546\n",
      "step: 89 and loss: 7.82742106112572\n",
      "step: 90 and loss: 7.703671784798315\n",
      "step: 91 and loss: 7.945110794011777\n",
      "step: 92 and loss: 7.912145445766043\n",
      "step: 93 and loss: 7.703838603042646\n",
      "step: 94 and loss: 7.799045091211122\n",
      "step: 95 and loss: 7.900868606631812\n",
      "step: 96 and loss: 7.643384125704376\n",
      "step: 97 and loss: 7.754920179770691\n",
      "step: 98 and loss: 7.690526805121849\n",
      "step: 99 and loss: 7.744710841322858\n",
      "step: 100 and loss: 7.572152157898906\n",
      "step: 101 and loss: 7.881536621216606\n",
      "step: 102 and loss: 7.7991369910913875\n",
      "step: 103 and loss: 7.688109979134785\n",
      "step: 104 and loss: 7.746825589587317\n",
      "step: 105 and loss: 7.668844288932796\n",
      "step: 106 and loss: 7.873171163247726\n",
      "step: 107 and loss: 7.78737191382325\n",
      "step: 108 and loss: 7.493309934303471\n",
      "step: 109 and loss: 7.652681211845208\n",
      "step: 110 and loss: 7.73749468538833\n",
      "step: 111 and loss: 7.814477971126807\n",
      "step: 112 and loss: 7.682539442109753\n",
      "step: 113 and loss: 7.760391905751521\n",
      "step: 114 and loss: 7.618218044284798\n",
      "step: 115 and loss: 7.729352592580189\n",
      "step: 116 and loss: 7.735478383896857\n",
      "step: 117 and loss: 7.7823095642715945\n",
      "step: 118 and loss: 7.686730160087445\n",
      "step: 119 and loss: 7.608579734234144\n",
      "step: 120 and loss: 7.772340462119121\n",
      "step: 121 and loss: 7.716614656212014\n",
      "step: 122 and loss: 7.99677845190687\n",
      "step: 123 and loss: 7.739965061886716\n",
      "step: 124 and loss: 7.524244992631654\n",
      "step: 125 and loss: 7.811387673132611\n",
      "step: 126 and loss: 7.9599060374499615\n",
      "step: 127 and loss: 7.758169360341408\n",
      "step: 128 and loss: 7.860346158240487\n",
      "step: 129 and loss: 7.486986895235379\n",
      "step: 130 and loss: 8.238523804344013\n",
      "step: 131 and loss: 7.726201540880265\n",
      "step: 132 and loss: 7.906296431162708\n",
      "step: 133 and loss: 7.771395927445844\n",
      "step: 134 and loss: 7.766627815652724\n",
      "step: 135 and loss: 8.31194578620642\n",
      "step: 136 and loss: 7.957999674243824\n",
      "step: 137 and loss: 8.117884035318822\n",
      "step: 138 and loss: 8.274707828191678\n",
      "step: 139 and loss: 8.07174353881778\n",
      "step: 140 and loss: 8.414547040051863\n",
      "step: 141 and loss: 8.17761440285975\n",
      "step: 142 and loss: 8.107359284735727\n",
      "step: 143 and loss: 8.004754053847817\n",
      "step: 144 and loss: 8.251468188039617\n",
      "step: 145 and loss: 8.00498463935707\n",
      "step: 146 and loss: 8.250813130110265\n",
      "step: 147 and loss: 8.358696158683006\n",
      "step: 148 and loss: 8.17143148135998\n",
      "step: 149 and loss: 8.292129597389165\n",
      "step: 150 and loss: 8.227040143374532\n",
      "step: 151 and loss: 8.746569042421235\n",
      "step: 152 and loss: 8.146256366172501\n",
      "step: 153 and loss: 8.729044615951167\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[911], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m targets \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(one_hot(y, \u001b[39mlen\u001b[39m(word_to_id)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m)) \u001b[39m# (B, T, C) transposed to (T, B, C)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> 7\u001b[0m outputs, losses \u001b[39m=\u001b[39m model(inputs, targets, no_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      8\u001b[0m model\u001b[39m.\u001b[39mbackward(outputs)\n\u001b[0;32m      9\u001b[0m \u001b[39m#optimizer.clip_gradient_v2(0.25)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[906], line 26\u001b[0m, in \u001b[0;36mLSTMModel.__call__\u001b[1;34m(self, x, y, no_grad)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, y, no_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 26\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x, y, no_grad)\n",
      "Cell \u001b[1;32mIn[906], line 38\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x, y, no_grad)\u001b[0m\n\u001b[0;32m     36\u001b[0m hidden, cell_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(cxt, hidden, cell_state, no_grad)\n\u001b[0;32m     37\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(hidden, no_grad)\n\u001b[1;32m---> 38\u001b[0m probas[timestamp], losses[timestamp] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcross_entropy(logits, y[timestamp], no_grad)\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_grad:\n\u001b[0;32m     40\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_dicts\u001b[39m.\u001b[39mappend((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding\u001b[39m.\u001b[39mforward_dict, \n\u001b[0;32m     41\u001b[0m                             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mforward_dict, \n\u001b[0;32m     42\u001b[0m                             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear\u001b[39m.\u001b[39mforward_dict, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m                             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mforget_gate\u001b[39m.\u001b[39mforward_dict, \n\u001b[0;32m     47\u001b[0m                             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mcell_state\u001b[39m.\u001b[39mforward_dict))                \n",
      "Cell \u001b[1;32mIn[703], line 146\u001b[0m, in \u001b[0;36mCriterion.__call__\u001b[1;34m(self, x, y, no_grad)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, y, no_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x, y, no_grad)\n",
      "Cell \u001b[1;32mIn[703], line 167\u001b[0m, in \u001b[0;36mCrossEntropy.forward\u001b[1;34m(self, x, y, no_grad)\u001b[0m\n\u001b[0;32m    165\u001b[0m p[np\u001b[39m.\u001b[39mwhere(p \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon)] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon\n\u001b[0;32m    166\u001b[0m p[np\u001b[39m.\u001b[39mwhere(p \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon)] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon\n\u001b[1;32m--> 167\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39msum(y \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mlog(p)) \u001b[39m/\u001b[39m m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_grad: \n\u001b[0;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m    170\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m: x,\n\u001b[0;32m    171\u001b[0m         \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m: y,\n\u001b[0;32m    172\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mp\u001b[39m\u001b[39m'\u001b[39m: p,\n\u001b[0;32m    173\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: loss,\n\u001b[0;32m    174\u001b[0m     }\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "for step, (x, y) in enumerate(ptb_iterator(train_data, batch_size, context_size)):\n",
    "    inputs = np.transpose(one_hot(x, len(word_to_id)), axes=(1,0,2)) # (B, T, C) transposed to (T, B, C)\n",
    "    targets = np.transpose(one_hot(y, len(word_to_id)), axes=(1,0,2)) # (B, T, C) transposed to (T, B, C)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs, losses = model(inputs, targets, no_grad=False)\n",
    "    model.backward(outputs)\n",
    "    #optimizer.clip_gradient_v2(0.25)\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 1 == 0:\n",
    "        print(f'step: {step} and loss: {np.mean(losses)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with pytorch RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, seq_len, hidden_size, batch_size, bias=True):\n",
    "        super(RNN, self).__init__()\n",
    "        self.vocab_size, self.emb_size, self.seq_len, self.hidden_size, self.batch_size, self.bias = vocab_size, emb_size, seq_len, hidden_size, batch_size, bias\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.hidden = nn.Linear(emb_size + hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize the embedding and output weights uniformly.\"\"\"\n",
    "        # Intialize embedding weights unformly in the range [-0.1, 0.1]\n",
    "        nn.init.uniform_(self.embeddings.weight, -0.1, 0.1)\n",
    "\n",
    "        # Initialize the weights and biases uniformly\n",
    "        b = 1/math.sqrt(self.hidden_size)\n",
    "        nn.init.uniform_(self.hidden.weight, -b, b)\n",
    "        nn.init.uniform_(self.hidden.bias, -b, b)\n",
    "\n",
    "        # Initialize output layer weights uniformly in the range [-0.1, 0.1]\n",
    "        # And all the biases to 0\n",
    "        nn.init.uniform_(self.output.weight, -0.1, 0.1)\n",
    "        nn.init.zeros_(self.output.bias)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \"\"\"Initialize the hidden states to zero.\n",
    "\n",
    "        This is used for the first mini-batch in an epoch, only.\n",
    "        \"\"\"\n",
    "        return torch.zeros(self.batch_size, self.hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        if x.is_cuda:\n",
    "            device = x.get_device()\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        # Create a tensor to store outputs during the Forward\n",
    "        outputs = torch.zeros(self.seq_len, self.batch_size, self.vocab_size).to(device)\n",
    "        for timestamp in range(self.seq_len):\n",
    "            cx = self.embeddings(x[timestamp])\n",
    "            combined = torch.cat((cx, hidden_state), 1)\n",
    "            pre_hidden = self.hidden(combined)\n",
    "            hidden = torch.tanh(pre_hidden)\n",
    "            outputs[timestamp] = self.output(hidden)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the GPU\n"
     ]
    }
   ],
   "source": [
    "# Use the GPU if you have one\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"WARNING: You are about to run on cpu, and this will likely run out \\\n",
    "      of memory. \\n You can try setting batch_size=1 to reduce memory usage\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size=128\n",
    "hidden_size=128\n",
    "seq_len=4\n",
    "batch_size=128\n",
    "vocab_size=10001\n",
    "#num_layers=1\n",
    "#dp_keep_prob=1\n",
    "initial_lr = 0.01\n",
    "debug = True\n",
    "num_epochs = 5\n",
    "#save_best = False\n",
    "#save_dir = \"/\"\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(emb_size=emb_size, hidden_size=hidden_size,\n",
    "            seq_len=seq_len, batch_size=batch_size,\n",
    "            vocab_size=vocab_size)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# LOSS FUNCTION\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "# LEARNING RATE\n",
    "lr = initial_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"\n",
    "    Wraps hidden states in new Tensors, to detach them from their history.\n",
    "\n",
    "    This prevents Pytorch from trying to backpropagate into previous input\n",
    "    sequences when we use the final hidden states from one mini-batch as the\n",
    "    initial hidden states for the next mini-batch.\n",
    "\n",
    "    Using the final hidden states in this way makes sense when the elements of\n",
    "    the mini-batches are actually successive subsequences in a set of longer sequences.\n",
    "    This is the case with the way we've processed the Penn Treebank dataset.\n",
    "    \"\"\"\n",
    "    if isinstance(h, Variable):\n",
    "        return h.detach_()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "def run_epoch(model, data, is_train=False, lr=1.0):\n",
    "    \"\"\"\n",
    "    One epoch of training/validation (depending on flag is_train).\n",
    "    \"\"\"\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    epoch_size = ((len(data) // model.batch_size) - 1) // model.seq_len\n",
    "    start_time = time.time()\n",
    "    hidden = model.init_hidden()\n",
    "    hidden = hidden.to(device)\n",
    "\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    losses = []\n",
    "\n",
    "    # LOOP THROUGH MINIBATCHES\n",
    "    for step, (x, y) in enumerate(ptb_iterator(data, model.batch_size, model.seq_len)):\n",
    "        inputs = torch.from_numpy(x.astype(np.int64)).transpose(0, 1).contiguous().to(device)\n",
    "        model.zero_grad()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "\n",
    "        targets = torch.from_numpy(y.astype(np.int64)).transpose(0, 1).contiguous().to(device)\n",
    "        tt = torch.squeeze(targets.view(-1, model.batch_size * model.seq_len))\n",
    "\n",
    "        # LOSS COMPUTATION\n",
    "        # This line currently averages across all the sequences in a mini-batch\n",
    "        # and all time-steps of the sequences.\n",
    "        # For problem 4.1, you will (instead) need to compute the average loss\n",
    "        # at each time-step separately. Hint: use the method retain_grad to keep\n",
    "        # gradients for intermediate nodes of the computational graph.\n",
    "        #\n",
    "        loss = loss_fn(outputs.contiguous().view(-1, model.vocab_size), tt)\n",
    "        costs += loss.data.item() * model.seq_len\n",
    "        losses.append(costs)\n",
    "        iters += model.seq_len\n",
    "        if debug:\n",
    "            print(step, loss.item())\n",
    "        if is_train:  # Only update parameters if training\n",
    "            loss.backward(retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "            optimizer.step()\n",
    "            if step % (epoch_size // 10) == 10:\n",
    "                print('step: '+ str(step) + '\\t' \\\n",
    "                    + \"loss (sum over all examples' seen this epoch):\" + str(costs) + '\\t' \\\n",
    "                    + 'speed (wps):' + str(iters * model.batch_size / (time.time() - start_time)))\n",
    "    return np.exp(costs / iters), losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## Running Main Loop ##########################\n",
      "\n",
      "EPOCH 0 ------------------\n",
      "training data shape is 971657 and the number of batches is 7591\n",
      "0 9.213391304016113\n",
      "1 9.215801239013672\n",
      "2 9.21054458618164\n",
      "3 9.215188980102539\n",
      "4 9.216226577758789\n",
      "5 9.214129447937012\n",
      "6 9.213945388793945\n",
      "7 9.212898254394531\n",
      "8 9.213916778564453\n",
      "9 9.207658767700195\n",
      "10 9.212010383605957\n",
      "step: 10\tloss (sum over all examples' seen this epoch):405.3828468322754\tspeed (wps):6642.81453548031\n",
      "11 9.206123352050781\n",
      "12 9.211172103881836\n",
      "13 9.213326454162598\n",
      "14 9.209976196289062\n",
      "15 9.20836353302002\n",
      "16 9.211698532104492\n",
      "17 9.207874298095703\n",
      "18 9.20516300201416\n",
      "19 9.207450866699219\n",
      "20 9.212579727172852\n",
      "21 9.205589294433594\n",
      "22 9.205440521240234\n",
      "23 9.203186988830566\n",
      "24 9.205988883972168\n",
      "25 9.209946632385254\n",
      "26 9.207103729248047\n",
      "27 9.207426071166992\n",
      "28 9.206121444702148\n",
      "29 9.20392894744873\n",
      "30 9.207456588745117\n",
      "31 9.20374584197998\n",
      "32 9.204540252685547\n",
      "33 9.209150314331055\n",
      "34 9.203968048095703\n",
      "35 9.203532218933105\n",
      "36 9.20254135131836\n",
      "37 9.20551872253418\n",
      "38 9.204598426818848\n",
      "39 9.201375007629395\n",
      "40 9.207383155822754\n",
      "41 9.202606201171875\n",
      "42 9.205141067504883\n",
      "43 9.201934814453125\n",
      "44 9.202118873596191\n",
      "45 9.203865051269531\n",
      "46 9.201979637145996\n",
      "47 9.20057201385498\n",
      "48 9.20063304901123\n",
      "49 9.199323654174805\n",
      "50 9.195324897766113\n",
      "51 9.198554992675781\n",
      "52 9.202669143676758\n",
      "53 9.198779106140137\n",
      "54 9.200300216674805\n",
      "55 9.200480461120605\n",
      "56 9.193033218383789\n",
      "57 9.195616722106934\n",
      "58 9.19663143157959\n",
      "59 9.20097541809082\n",
      "60 9.19428825378418\n",
      "61 9.19953441619873\n",
      "62 9.20046615600586\n",
      "63 9.194742202758789\n",
      "64 9.198180198669434\n",
      "65 9.195530891418457\n",
      "66 9.197301864624023\n",
      "67 9.194782257080078\n",
      "68 9.196488380432129\n",
      "69 9.195987701416016\n",
      "70 9.193166732788086\n",
      "71 9.193352699279785\n",
      "72 9.193121910095215\n",
      "73 9.191516876220703\n",
      "74 9.190661430358887\n",
      "75 9.194777488708496\n",
      "76 9.191302299499512\n",
      "77 9.192217826843262\n",
      "78 9.192635536193848\n",
      "79 9.196172714233398\n",
      "80 9.193574905395508\n",
      "81 9.187650680541992\n",
      "82 9.19014835357666\n",
      "83 9.196606636047363\n",
      "84 9.190738677978516\n",
      "85 9.190908432006836\n",
      "86 9.192957878112793\n",
      "87 9.18722152709961\n",
      "88 9.190130233764648\n",
      "89 9.187126159667969\n",
      "90 9.185208320617676\n",
      "91 9.195826530456543\n",
      "92 9.186823844909668\n",
      "93 9.188102722167969\n",
      "94 9.184527397155762\n",
      "95 9.190994262695312\n",
      "96 9.188556671142578\n",
      "97 9.1845703125\n",
      "98 9.189085006713867\n",
      "99 9.186437606811523\n",
      "100 9.181513786315918\n",
      "101 9.186514854431152\n",
      "102 9.183252334594727\n",
      "103 9.180802345275879\n",
      "104 9.185534477233887\n",
      "105 9.181525230407715\n",
      "106 9.18149185180664\n",
      "107 9.186593055725098\n",
      "108 9.176207542419434\n",
      "109 9.179356575012207\n",
      "110 9.180136680603027\n",
      "111 9.185914993286133\n",
      "112 9.177391052246094\n",
      "113 9.180082321166992\n",
      "114 9.18260669708252\n",
      "115 9.18265151977539\n",
      "116 9.179203033447266\n",
      "117 9.177515983581543\n",
      "118 9.183727264404297\n",
      "119 9.181474685668945\n",
      "120 9.184709548950195\n",
      "121 9.17940616607666\n",
      "122 9.182124137878418\n",
      "123 9.181204795837402\n",
      "124 9.17318058013916\n",
      "125 9.178595542907715\n",
      "126 9.177987098693848\n",
      "127 9.17896556854248\n",
      "128 9.17801284790039\n",
      "129 9.175070762634277\n",
      "130 9.17801570892334\n",
      "131 9.172457695007324\n",
      "132 9.175894737243652\n",
      "133 9.168572425842285\n",
      "134 9.176291465759277\n",
      "135 9.176088333129883\n",
      "136 9.174785614013672\n",
      "137 9.174219131469727\n",
      "138 9.175370216369629\n",
      "139 9.172173500061035\n",
      "140 9.174771308898926\n",
      "141 9.172496795654297\n",
      "142 9.172822952270508\n",
      "143 9.173012733459473\n",
      "144 9.167951583862305\n",
      "145 9.168283462524414\n",
      "146 9.165128707885742\n",
      "147 9.16766357421875\n",
      "148 9.168622970581055\n",
      "149 9.168472290039062\n",
      "150 9.171592712402344\n",
      "151 9.167194366455078\n",
      "152 9.165120124816895\n",
      "153 9.17192268371582\n",
      "154 9.168195724487305\n",
      "155 9.16792106628418\n",
      "156 9.167983055114746\n",
      "157 9.164666175842285\n",
      "158 9.167656898498535\n",
      "159 9.164098739624023\n",
      "160 9.170940399169922\n",
      "161 9.16097640991211\n",
      "162 9.171029090881348\n",
      "163 9.16572093963623\n",
      "164 9.164624214172363\n",
      "165 9.164371490478516\n",
      "166 9.162818908691406\n",
      "167 9.169638633728027\n",
      "168 9.164770126342773\n",
      "169 9.167749404907227\n",
      "170 9.164958953857422\n",
      "171 9.161349296569824\n",
      "172 9.159037590026855\n",
      "173 9.1631498336792\n",
      "174 9.161669731140137\n",
      "175 9.163037300109863\n",
      "176 9.15894603729248\n",
      "177 9.159809112548828\n",
      "178 9.154638290405273\n",
      "179 9.163530349731445\n",
      "180 9.162150382995605\n",
      "181 9.158522605895996\n",
      "182 9.16304874420166\n",
      "183 9.165284156799316\n",
      "184 9.164764404296875\n",
      "185 9.159513473510742\n",
      "186 9.158899307250977\n",
      "187 9.161469459533691\n",
      "188 9.153875350952148\n",
      "189 9.155080795288086\n",
      "190 9.162759780883789\n",
      "191 9.153520584106445\n",
      "192 9.157690048217773\n",
      "193 9.162339210510254\n",
      "194 9.162445068359375\n",
      "195 9.15970230102539\n",
      "196 9.146239280700684\n",
      "197 9.153111457824707\n",
      "198 9.15643310546875\n",
      "199 9.156232833862305\n",
      "step: 199\tloss (sum over all examples' seen this epoch):7348.079978942871\tspeed (wps):15626.279476408323\n",
      "200 9.157365798950195\n",
      "201 9.15419864654541\n",
      "202 9.159889221191406\n",
      "203 9.147438049316406\n",
      "204 9.157838821411133\n",
      "205 9.152236938476562\n",
      "206 9.150294303894043\n",
      "207 9.153547286987305\n",
      "208 9.15461540222168\n",
      "209 9.15012264251709\n",
      "210 9.150099754333496\n",
      "211 9.148757934570312\n",
      "212 9.152714729309082\n",
      "213 9.149767875671387\n",
      "214 9.146926879882812\n",
      "215 9.140416145324707\n",
      "216 9.141199111938477\n",
      "217 9.150213241577148\n",
      "218 9.142285346984863\n",
      "219 9.15268325805664\n",
      "220 9.149638175964355\n",
      "221 9.150232315063477\n",
      "222 9.145722389221191\n",
      "223 9.1482572555542\n",
      "224 9.150620460510254\n",
      "225 9.146188735961914\n",
      "226 9.148571968078613\n",
      "227 9.1484956741333\n",
      "228 9.144697189331055\n",
      "229 9.143472671508789\n",
      "230 9.141782760620117\n",
      "231 9.150927543640137\n",
      "232 9.133938789367676\n",
      "233 9.142677307128906\n",
      "234 9.145286560058594\n",
      "235 9.135729789733887\n",
      "236 9.141814231872559\n",
      "237 9.139143943786621\n",
      "238 9.138960838317871\n",
      "239 9.139908790588379\n",
      "240 9.138492584228516\n",
      "241 9.132081985473633\n",
      "242 9.136484146118164\n",
      "243 9.14937686920166\n",
      "244 9.132895469665527\n",
      "245 9.139508247375488\n",
      "246 9.14668083190918\n",
      "247 9.141857147216797\n",
      "248 9.139711380004883\n",
      "249 9.139240264892578\n",
      "250 9.13878345489502\n",
      "251 9.135790824890137\n",
      "252 9.137127876281738\n",
      "253 9.140547752380371\n",
      "254 9.13825798034668\n",
      "255 9.134492874145508\n",
      "256 9.130111694335938\n",
      "257 9.136404037475586\n",
      "258 9.132899284362793\n",
      "259 9.130151748657227\n",
      "260 9.12215805053711\n",
      "261 9.135147094726562\n",
      "262 9.129975318908691\n",
      "263 9.131609916687012\n",
      "264 9.130744934082031\n",
      "265 9.136697769165039\n",
      "266 9.121803283691406\n",
      "267 9.138797760009766\n",
      "268 9.13046646118164\n",
      "269 9.124520301818848\n",
      "270 9.1386137008667\n",
      "271 9.130558967590332\n",
      "272 9.128979682922363\n",
      "273 9.121622085571289\n",
      "274 9.129043579101562\n",
      "275 9.127458572387695\n",
      "276 9.128795623779297\n",
      "277 9.131885528564453\n",
      "278 9.119186401367188\n",
      "279 9.128854751586914\n",
      "280 9.132977485656738\n",
      "281 9.12857723236084\n",
      "282 9.124798774719238\n",
      "283 9.119588851928711\n",
      "284 9.130472183227539\n",
      "285 9.107897758483887\n",
      "286 9.117555618286133\n",
      "287 9.121048927307129\n",
      "288 9.119196891784668\n",
      "289 9.119935989379883\n",
      "290 9.129082679748535\n",
      "291 9.115450859069824\n",
      "292 9.115689277648926\n",
      "293 9.122100830078125\n",
      "294 9.106017112731934\n",
      "295 9.13110065460205\n",
      "296 9.126667976379395\n",
      "297 9.122474670410156\n",
      "298 9.118019104003906\n",
      "299 9.119653701782227\n",
      "300 9.115732192993164\n",
      "301 9.120924949645996\n",
      "302 9.127164840698242\n",
      "303 9.11963939666748\n",
      "304 9.113227844238281\n",
      "305 9.132095336914062\n",
      "306 9.119871139526367\n",
      "307 9.123714447021484\n",
      "308 9.108416557312012\n",
      "309 9.118730545043945\n",
      "310 9.106254577636719\n",
      "311 9.120687484741211\n",
      "312 9.10391616821289\n",
      "313 9.109644889831543\n",
      "314 9.110325813293457\n",
      "315 9.11766529083252\n",
      "316 9.122886657714844\n",
      "317 9.115066528320312\n",
      "318 9.115836143493652\n",
      "319 9.124197006225586\n",
      "320 9.099281311035156\n",
      "321 9.113966941833496\n",
      "322 9.110466003417969\n",
      "323 9.106693267822266\n",
      "324 9.099546432495117\n",
      "325 9.11561107635498\n",
      "326 9.105256080627441\n",
      "327 9.117783546447754\n",
      "328 9.104052543640137\n",
      "329 9.104833602905273\n",
      "330 9.111268997192383\n",
      "331 9.109350204467773\n",
      "332 9.103981018066406\n",
      "333 9.103439331054688\n",
      "334 9.101868629455566\n",
      "335 9.104852676391602\n",
      "336 9.104884147644043\n",
      "337 9.097223281860352\n",
      "338 9.10300064086914\n",
      "339 9.11195182800293\n",
      "340 9.101545333862305\n",
      "341 9.091520309448242\n",
      "342 9.109002113342285\n",
      "343 9.113199234008789\n",
      "344 9.100348472595215\n",
      "345 9.100937843322754\n",
      "346 9.104719161987305\n",
      "347 9.108012199401855\n",
      "348 9.10387134552002\n",
      "349 9.106348991394043\n",
      "350 9.091064453125\n",
      "351 9.093964576721191\n",
      "352 9.094768524169922\n",
      "353 9.094330787658691\n",
      "354 9.10346508026123\n",
      "355 9.092180252075195\n",
      "356 9.086685180664062\n",
      "357 9.098366737365723\n",
      "358 9.100764274597168\n",
      "359 9.095213890075684\n",
      "360 9.075715065002441\n",
      "361 9.105541229248047\n",
      "362 9.098254203796387\n",
      "363 9.07865047454834\n",
      "364 9.090227127075195\n",
      "365 9.099617958068848\n",
      "366 9.088716506958008\n",
      "367 9.086021423339844\n",
      "368 9.081598281860352\n",
      "369 9.086091041564941\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[803], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEPOCH \u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(epoch)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m ------------------\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[39m# RUN MODEL ON TRAINING DATA\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m train_ppl, train_loss \u001b[39m=\u001b[39m run_epoch(model, train_data, \u001b[39mTrue\u001b[39;49;00m, lr)\n\u001b[0;32m     23\u001b[0m \u001b[39m# RUN MODEL ON VALIDATION DATA\u001b[39;00m\n\u001b[0;32m     24\u001b[0m val_ppl, val_loss \u001b[39m=\u001b[39m run_epoch(model, valid_data)\n",
      "Cell \u001b[1;32mIn[802], line 59\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(model, data, is_train, lr)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[39mprint\u001b[39m(step, loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     58\u001b[0m \u001b[39mif\u001b[39;00m is_train:  \u001b[39m# Only update parameters if training\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     60\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m0.25\u001b[39m)\n\u001b[0;32m     61\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\zarou\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zarou\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n########## Running Main Loop ##########################\")\n",
    "train_ppls = []\n",
    "train_losses = []\n",
    "val_ppls = []\n",
    "val_losses = []\n",
    "best_val_so_far = np.inf\n",
    "times = []\n",
    "\n",
    "# In debug mode, only run one epoch\n",
    "if debug:\n",
    "    num_epochs = 1\n",
    "else:\n",
    "    num_epochs = num_epochs\n",
    "\n",
    "# MAIN LOOP\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    print('\\nEPOCH '+str(epoch)+' ------------------')\n",
    "    \n",
    "    # RUN MODEL ON TRAINING DATA\n",
    "    train_ppl, train_loss = run_epoch(model, train_data, True, lr)\n",
    "\n",
    "    # RUN MODEL ON VALIDATION DATA\n",
    "    val_ppl, val_loss = run_epoch(model, valid_data)\n",
    "\n",
    "\n",
    "    # SAVE MODEL IF IT'S THE BEST SO FAR\n",
    "    if val_ppl < best_val_so_far:\n",
    "        best_val_so_far = val_ppl\n",
    "\n",
    "    # LOC RESULTS\n",
    "    train_ppls.append(train_ppl)\n",
    "    val_ppls.append(val_ppl)\n",
    "    train_losses.extend(train_loss)\n",
    "    val_losses.extend(val_loss)\n",
    "    times.append(time.time() - t0)\n",
    "    log_str = 'epoch: ' + str(epoch) + '\\t' \\\n",
    "            + 'train ppl: ' + str(train_ppl) + '\\t' \\\n",
    "            + 'val ppl: ' + str(val_ppl)  + '\\t' \\\n",
    "            + 'best val: ' + str(best_val_so_far) + '\\t' \\\n",
    "            + 'time (s) spent in epoch: ' + str(times[-1])\n",
    "    print(log_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the GPU\n"
     ]
    }
   ],
   "source": [
    "# Use the GPU if you have one\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"WARNING: You are about to run on cpu, and this will likely run out \\\n",
    "      of memory. \\n You can try setting batch_size=1 to reduce memory usage\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage(h):\n",
    "    \"\"\"\n",
    "    Wraps hidden states in new Tensors, to detach them from their history.\n",
    "\n",
    "    This prevents Pytorch from trying to backpropagate into previous input\n",
    "    sequences when we use the final hidden states from one mini-batch as the\n",
    "    initial hidden states for the next mini-batch.\n",
    "\n",
    "    Using the final hidden states in this way makes sense when the elements of\n",
    "    the mini-batches are actually successive subsequences in a set of longer sequences.\n",
    "    This is the case with the way we've processed the Penn Treebank dataset.\n",
    "    \"\"\"\n",
    "    if isinstance(h, Variable):\n",
    "        return h.detach_()\n",
    "    else:\n",
    "        return tuple(repackage(v) for v in h)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, seq_len, hidden_size, batch_size, bias=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.vocab_size, self.emb_size, self.seq_len, self.hidden_size, self.batch_size, self.bias = vocab_size, emb_size, seq_len, hidden_size, batch_size, bias\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.rnn = nn.LSTM(emb_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def init_hidden_and_cell(self):\n",
    "        \"\"\"Initialize the hidden states to zero.\n",
    "\n",
    "        This is used for the first mini-batch in an epoch, only.\n",
    "        \"\"\"\n",
    "        return torch.zeros(1, self.batch_size, self.hidden_size), torch.zeros(1, self.batch_size, self.hidden_size)\n",
    "    \n",
    "    def forward(self, x, h_1, c_1):\n",
    "        x = x.to(device)\n",
    "        cx = self.embeddings(x)\n",
    "        output, (h, c) = self.rnn(cx, (h_1, c_1))\n",
    "        outputs = self.output(output).to(device)\n",
    "        return outputs, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size=128\n",
    "hidden_size=128\n",
    "seq_len=4\n",
    "batch_size=128\n",
    "vocab_size=10001\n",
    "lr = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "model = LSTM(emb_size=emb_size, hidden_size=hidden_size,\n",
    "            seq_len=seq_len, batch_size=batch_size,\n",
    "            vocab_size=vocab_size)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# LOSS FUNCTION\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################### EPOCH 0 #########################\n",
      "training data shape is 971657 and the number of batches is 7591\n",
      "step: 0 loss: 9.208250045776367\n",
      "step: 1 loss: 9.178266525268555\n",
      "step: 2 loss: 9.154075622558594\n",
      "step: 3 loss: 9.094618797302246\n",
      "step: 4 loss: 9.034873962402344\n",
      "step: 5 loss: 8.936956405639648\n",
      "step: 6 loss: 8.889568328857422\n",
      "step: 7 loss: 8.86879825592041\n",
      "step: 8 loss: 8.550055503845215\n",
      "step: 9 loss: 8.076891899108887\n",
      "step: 10 loss: 7.584944725036621\n",
      "step: 11 loss: 7.0122857093811035\n",
      "step: 12 loss: 7.219669342041016\n",
      "step: 13 loss: 6.65168571472168\n",
      "step: 14 loss: 6.986882209777832\n",
      "step: 15 loss: 6.6848883628845215\n",
      "step: 16 loss: 7.011778831481934\n",
      "step: 17 loss: 6.733390808105469\n",
      "step: 18 loss: 6.834959506988525\n",
      "step: 19 loss: 6.762497425079346\n",
      "step: 20 loss: 6.74285364151001\n",
      "step: 21 loss: 6.373056411743164\n",
      "step: 22 loss: 6.451379776000977\n",
      "step: 23 loss: 6.67000150680542\n",
      "step: 24 loss: 6.749824523925781\n",
      "step: 25 loss: 6.369228363037109\n",
      "step: 26 loss: 6.415034770965576\n",
      "step: 27 loss: 6.360847473144531\n",
      "step: 28 loss: 6.582243919372559\n",
      "step: 29 loss: 6.5840606689453125\n",
      "step: 30 loss: 6.674497127532959\n",
      "step: 31 loss: 6.33599853515625\n",
      "step: 32 loss: 6.159285545349121\n",
      "step: 33 loss: 6.5728840827941895\n",
      "step: 34 loss: 6.2425689697265625\n",
      "step: 35 loss: 6.358053684234619\n",
      "step: 36 loss: 6.4937591552734375\n",
      "step: 37 loss: 6.30957555770874\n",
      "step: 38 loss: 6.290969371795654\n",
      "step: 39 loss: 6.385435104370117\n",
      "step: 40 loss: 6.273468017578125\n",
      "step: 41 loss: 6.202680587768555\n",
      "step: 42 loss: 6.104550838470459\n",
      "step: 43 loss: 6.23626184463501\n",
      "step: 44 loss: 6.155417442321777\n",
      "step: 45 loss: 6.407754421234131\n",
      "step: 46 loss: 6.2719855308532715\n",
      "step: 47 loss: 6.078889846801758\n",
      "step: 48 loss: 6.564663410186768\n",
      "step: 49 loss: 6.049972057342529\n",
      "step: 50 loss: 6.2776007652282715\n",
      "step: 51 loss: 6.048058986663818\n",
      "step: 52 loss: 6.336633205413818\n",
      "step: 53 loss: 6.3704400062561035\n",
      "step: 54 loss: 6.177959442138672\n",
      "step: 55 loss: 6.054971218109131\n",
      "step: 56 loss: 6.245875358581543\n",
      "step: 57 loss: 6.074017524719238\n",
      "step: 58 loss: 6.100464344024658\n",
      "step: 59 loss: 6.226262092590332\n",
      "step: 60 loss: 6.3109564781188965\n",
      "step: 61 loss: 6.18121337890625\n",
      "step: 62 loss: 6.4250054359436035\n",
      "step: 63 loss: 6.027064323425293\n",
      "step: 64 loss: 6.112311840057373\n",
      "step: 65 loss: 6.157528877258301\n",
      "step: 66 loss: 6.143239974975586\n",
      "step: 67 loss: 6.273586273193359\n",
      "step: 68 loss: 6.060758113861084\n",
      "step: 69 loss: 6.007143020629883\n",
      "step: 70 loss: 5.851197719573975\n",
      "step: 71 loss: 6.361469745635986\n",
      "step: 72 loss: 6.224458694458008\n",
      "step: 73 loss: 5.848324298858643\n",
      "step: 74 loss: 5.893844127655029\n",
      "step: 75 loss: 6.121709823608398\n",
      "step: 76 loss: 6.255308151245117\n",
      "step: 77 loss: 6.122870445251465\n",
      "step: 78 loss: 6.062597274780273\n",
      "step: 79 loss: 6.221834182739258\n",
      "step: 80 loss: 6.280256748199463\n",
      "step: 81 loss: 6.147001266479492\n",
      "step: 82 loss: 5.9672932624816895\n",
      "step: 83 loss: 6.002762794494629\n",
      "step: 84 loss: 5.912092685699463\n",
      "step: 85 loss: 6.032922744750977\n",
      "step: 86 loss: 6.042989730834961\n",
      "step: 87 loss: 6.149264812469482\n",
      "step: 88 loss: 6.330921173095703\n",
      "step: 89 loss: 6.007779121398926\n",
      "step: 90 loss: 5.864139556884766\n",
      "step: 91 loss: 6.2760329246521\n",
      "step: 92 loss: 6.135813236236572\n",
      "step: 93 loss: 5.94142484664917\n",
      "step: 94 loss: 6.077837944030762\n",
      "step: 95 loss: 6.1857590675354\n",
      "step: 96 loss: 5.695742130279541\n",
      "step: 97 loss: 6.042117595672607\n",
      "step: 98 loss: 5.7887282371521\n",
      "step: 99 loss: 6.005871772766113\n",
      "step: 100 loss: 5.718256950378418\n",
      "step: 101 loss: 6.069962978363037\n",
      "step: 102 loss: 6.03599214553833\n",
      "step: 103 loss: 6.034696578979492\n",
      "step: 104 loss: 5.974678039550781\n",
      "step: 105 loss: 5.775251388549805\n",
      "step: 106 loss: 6.10938835144043\n",
      "step: 107 loss: 6.096564769744873\n",
      "step: 108 loss: 5.842620849609375\n",
      "step: 109 loss: 5.958354949951172\n",
      "step: 110 loss: 5.879988193511963\n",
      "step: 111 loss: 6.112359523773193\n",
      "step: 112 loss: 5.967935562133789\n",
      "step: 113 loss: 5.975865840911865\n",
      "step: 114 loss: 6.077549934387207\n",
      "step: 115 loss: 6.2576584815979\n",
      "step: 116 loss: 5.886075973510742\n",
      "step: 117 loss: 5.884927749633789\n",
      "step: 118 loss: 5.916189193725586\n",
      "step: 119 loss: 5.96343994140625\n",
      "step: 120 loss: 5.8235650062561035\n",
      "step: 121 loss: 5.956589221954346\n",
      "step: 122 loss: 6.113454818725586\n",
      "step: 123 loss: 5.895042896270752\n",
      "step: 124 loss: 5.711594104766846\n",
      "step: 125 loss: 5.972408771514893\n",
      "step: 126 loss: 6.17968225479126\n",
      "step: 127 loss: 5.878395080566406\n",
      "step: 128 loss: 5.844461441040039\n",
      "step: 129 loss: 5.725934028625488\n",
      "step: 130 loss: 6.081504821777344\n",
      "step: 131 loss: 5.891931056976318\n",
      "step: 132 loss: 6.009804725646973\n",
      "step: 133 loss: 5.901517868041992\n",
      "step: 134 loss: 5.757072448730469\n",
      "step: 135 loss: 6.2316412925720215\n",
      "step: 136 loss: 5.98246955871582\n",
      "step: 137 loss: 6.063663482666016\n",
      "step: 138 loss: 6.071239471435547\n",
      "step: 139 loss: 5.870550155639648\n",
      "step: 140 loss: 6.072750091552734\n",
      "step: 141 loss: 6.064455509185791\n",
      "step: 142 loss: 5.895353317260742\n",
      "step: 143 loss: 5.831785202026367\n",
      "step: 144 loss: 6.032908916473389\n",
      "step: 145 loss: 5.781163215637207\n",
      "step: 146 loss: 5.855224132537842\n",
      "step: 147 loss: 5.993181228637695\n",
      "step: 148 loss: 5.8192009925842285\n",
      "step: 149 loss: 5.877417087554932\n",
      "step: 150 loss: 5.866384029388428\n",
      "step: 151 loss: 5.889250755310059\n",
      "step: 152 loss: 5.578976631164551\n",
      "step: 153 loss: 6.153932571411133\n",
      "step: 154 loss: 5.80957555770874\n",
      "step: 155 loss: 5.755908489227295\n",
      "step: 156 loss: 5.865841388702393\n",
      "step: 157 loss: 5.7895965576171875\n",
      "step: 158 loss: 5.75282621383667\n",
      "step: 159 loss: 5.6037373542785645\n",
      "step: 160 loss: 5.985342502593994\n",
      "step: 161 loss: 5.630337238311768\n",
      "step: 162 loss: 6.056945323944092\n",
      "step: 163 loss: 6.048449516296387\n",
      "step: 164 loss: 5.863353252410889\n",
      "step: 165 loss: 5.978623390197754\n",
      "step: 166 loss: 5.776573181152344\n",
      "step: 167 loss: 6.106060981750488\n",
      "step: 168 loss: 5.948668003082275\n",
      "step: 169 loss: 5.836635589599609\n",
      "step: 170 loss: 6.002891540527344\n",
      "step: 171 loss: 5.67759895324707\n",
      "step: 172 loss: 5.590645790100098\n",
      "step: 173 loss: 5.811069011688232\n",
      "step: 174 loss: 5.700054168701172\n",
      "step: 175 loss: 5.853725910186768\n",
      "step: 176 loss: 5.978403091430664\n",
      "step: 177 loss: 5.734171390533447\n",
      "step: 178 loss: 5.720837116241455\n",
      "step: 179 loss: 6.155398368835449\n",
      "step: 180 loss: 5.658359050750732\n",
      "step: 181 loss: 5.843260288238525\n",
      "step: 182 loss: 5.687222003936768\n",
      "step: 183 loss: 5.961965084075928\n",
      "step: 184 loss: 5.857977867126465\n",
      "step: 185 loss: 5.792910099029541\n",
      "step: 186 loss: 5.803523063659668\n",
      "step: 187 loss: 5.966010093688965\n",
      "step: 188 loss: 5.685272693634033\n",
      "step: 189 loss: 5.652783393859863\n",
      "step: 190 loss: 5.816648483276367\n",
      "step: 191 loss: 5.574954509735107\n",
      "step: 192 loss: 5.793560028076172\n",
      "step: 193 loss: 6.106694221496582\n",
      "step: 194 loss: 5.949682235717773\n",
      "step: 195 loss: 5.719335079193115\n",
      "step: 196 loss: 5.696439266204834\n",
      "step: 197 loss: 5.706115245819092\n",
      "step: 198 loss: 5.542169094085693\n",
      "step: 199 loss: 5.749256610870361\n",
      "step: 200 loss: 6.06028413772583\n",
      "step: 201 loss: 5.838339805603027\n",
      "step: 202 loss: 5.900423526763916\n",
      "step: 203 loss: 5.596388339996338\n",
      "step: 204 loss: 5.702667236328125\n",
      "step: 205 loss: 5.831292152404785\n",
      "step: 206 loss: 5.790682792663574\n",
      "step: 207 loss: 5.658569812774658\n",
      "step: 208 loss: 5.57681131362915\n",
      "step: 209 loss: 5.616699695587158\n",
      "step: 210 loss: 5.7996392250061035\n",
      "step: 211 loss: 5.8598408699035645\n",
      "step: 212 loss: 5.828766345977783\n",
      "step: 213 loss: 5.743581295013428\n",
      "step: 214 loss: 5.635651111602783\n",
      "step: 215 loss: 5.641666889190674\n",
      "step: 216 loss: 5.415340900421143\n",
      "step: 217 loss: 5.598546504974365\n",
      "step: 218 loss: 5.593844890594482\n",
      "step: 219 loss: 5.689972877502441\n",
      "step: 220 loss: 5.717109203338623\n",
      "step: 221 loss: 5.7003302574157715\n",
      "step: 222 loss: 5.539758205413818\n",
      "step: 223 loss: 5.624582290649414\n",
      "step: 224 loss: 5.944701671600342\n",
      "step: 225 loss: 5.851930141448975\n",
      "step: 226 loss: 5.832979202270508\n",
      "step: 227 loss: 5.962224960327148\n",
      "step: 228 loss: 5.64806604385376\n",
      "step: 229 loss: 5.777220726013184\n",
      "step: 230 loss: 5.806039810180664\n",
      "step: 231 loss: 5.722970485687256\n",
      "step: 232 loss: 5.469563961029053\n",
      "step: 233 loss: 5.839138031005859\n",
      "step: 234 loss: 5.568541049957275\n",
      "step: 235 loss: 5.565661430358887\n",
      "step: 236 loss: 5.710417747497559\n",
      "step: 237 loss: 5.770811557769775\n",
      "step: 238 loss: 5.629615306854248\n",
      "step: 239 loss: 5.675160884857178\n",
      "step: 240 loss: 5.864762306213379\n",
      "step: 241 loss: 5.614409923553467\n",
      "step: 242 loss: 5.660661697387695\n",
      "step: 243 loss: 5.959804058074951\n",
      "step: 244 loss: 5.570129871368408\n",
      "step: 245 loss: 5.588382244110107\n",
      "step: 246 loss: 5.7219624519348145\n",
      "step: 247 loss: 5.701336860656738\n",
      "step: 248 loss: 5.888117790222168\n",
      "step: 249 loss: 5.800852298736572\n",
      "step: 250 loss: 5.489715099334717\n",
      "step: 251 loss: 5.432572841644287\n",
      "step: 252 loss: 5.594037055969238\n",
      "step: 253 loss: 5.810266971588135\n",
      "step: 254 loss: 5.695009231567383\n",
      "step: 255 loss: 5.558290958404541\n",
      "step: 256 loss: 5.645450592041016\n",
      "step: 257 loss: 5.694160461425781\n",
      "step: 258 loss: 5.761794090270996\n",
      "step: 259 loss: 5.507887840270996\n",
      "step: 260 loss: 5.320968151092529\n",
      "step: 261 loss: 5.591631889343262\n",
      "step: 262 loss: 5.622729301452637\n",
      "step: 263 loss: 5.574881076812744\n",
      "step: 264 loss: 5.452070713043213\n",
      "step: 265 loss: 5.833869934082031\n",
      "step: 266 loss: 5.39056396484375\n",
      "step: 267 loss: 5.679016590118408\n",
      "step: 268 loss: 5.75152587890625\n",
      "step: 269 loss: 5.507540702819824\n",
      "step: 270 loss: 5.903947353363037\n",
      "step: 271 loss: 5.641850471496582\n",
      "step: 272 loss: 5.472082614898682\n",
      "step: 273 loss: 5.401664733886719\n",
      "step: 274 loss: 5.512569904327393\n",
      "step: 275 loss: 5.457622051239014\n",
      "step: 276 loss: 5.477825164794922\n",
      "step: 277 loss: 5.731106281280518\n",
      "step: 278 loss: 5.353093147277832\n",
      "step: 279 loss: 5.810688495635986\n",
      "step: 280 loss: 5.750816345214844\n",
      "step: 281 loss: 5.742121696472168\n",
      "step: 282 loss: 5.606490135192871\n",
      "step: 283 loss: 5.477794647216797\n",
      "step: 284 loss: 5.613069534301758\n",
      "step: 285 loss: 5.0915446281433105\n",
      "step: 286 loss: 5.132775783538818\n",
      "step: 287 loss: 5.519701957702637\n",
      "step: 288 loss: 5.572168827056885\n",
      "step: 289 loss: 5.410727500915527\n",
      "step: 290 loss: 5.611048221588135\n",
      "step: 291 loss: 5.656328201293945\n",
      "step: 292 loss: 5.453258991241455\n",
      "step: 293 loss: 5.828851699829102\n",
      "step: 294 loss: 5.54903507232666\n",
      "step: 295 loss: 5.733097553253174\n",
      "step: 296 loss: 5.751502990722656\n",
      "step: 297 loss: 5.714626312255859\n",
      "step: 298 loss: 5.380651950836182\n",
      "step: 299 loss: 5.610595226287842\n",
      "step: 300 loss: 5.578323841094971\n",
      "step: 301 loss: 5.649035930633545\n",
      "step: 302 loss: 6.1177592277526855\n",
      "step: 303 loss: 5.568685531616211\n",
      "step: 304 loss: 5.550447463989258\n",
      "step: 305 loss: 5.709259033203125\n",
      "step: 306 loss: 5.600281238555908\n",
      "step: 307 loss: 5.864306926727295\n",
      "step: 308 loss: 5.44340705871582\n",
      "step: 309 loss: 5.5813212394714355\n",
      "step: 310 loss: 5.423170566558838\n",
      "step: 311 loss: 5.679046630859375\n",
      "step: 312 loss: 5.3717546463012695\n",
      "step: 313 loss: 5.3466691970825195\n",
      "step: 314 loss: 5.407927513122559\n",
      "step: 315 loss: 5.480754375457764\n",
      "step: 316 loss: 5.712566375732422\n",
      "step: 317 loss: 5.69974422454834\n",
      "step: 318 loss: 5.561518669128418\n",
      "step: 319 loss: 5.602850437164307\n",
      "step: 320 loss: 5.309358596801758\n",
      "step: 321 loss: 5.514323711395264\n",
      "step: 322 loss: 5.779107093811035\n",
      "step: 323 loss: 5.6090407371521\n",
      "step: 324 loss: 5.350028038024902\n",
      "step: 325 loss: 5.608250617980957\n",
      "step: 326 loss: 5.413422584533691\n",
      "step: 327 loss: 5.509829998016357\n",
      "step: 328 loss: 5.544826507568359\n",
      "step: 329 loss: 5.558558940887451\n",
      "step: 330 loss: 5.642561435699463\n",
      "step: 331 loss: 5.441190242767334\n",
      "step: 332 loss: 5.2433953285217285\n",
      "step: 333 loss: 5.309189796447754\n",
      "step: 334 loss: 5.391686916351318\n",
      "step: 335 loss: 5.624992847442627\n",
      "step: 336 loss: 5.625712871551514\n",
      "step: 337 loss: 5.492949485778809\n",
      "step: 338 loss: 5.3386735916137695\n",
      "step: 339 loss: 5.6972808837890625\n",
      "step: 340 loss: 5.490242958068848\n",
      "step: 341 loss: 5.625060081481934\n",
      "step: 342 loss: 5.753457069396973\n",
      "step: 343 loss: 5.833924293518066\n",
      "step: 344 loss: 5.426642417907715\n",
      "step: 345 loss: 5.547860622406006\n",
      "step: 346 loss: 5.421167850494385\n",
      "step: 347 loss: 5.6750807762146\n",
      "step: 348 loss: 5.5300822257995605\n",
      "step: 349 loss: 5.694369792938232\n",
      "step: 350 loss: 5.578118324279785\n",
      "step: 351 loss: 5.353619575500488\n",
      "step: 352 loss: 5.66024923324585\n",
      "step: 353 loss: 5.527260780334473\n",
      "step: 354 loss: 5.634659767150879\n",
      "step: 355 loss: 5.377685546875\n",
      "step: 356 loss: 5.326566219329834\n",
      "step: 357 loss: 5.553411960601807\n",
      "step: 358 loss: 5.480088233947754\n",
      "step: 359 loss: 5.574608325958252\n",
      "step: 360 loss: 5.304711818695068\n",
      "step: 361 loss: 5.790052890777588\n",
      "step: 362 loss: 5.661218643188477\n",
      "step: 363 loss: 5.4849419593811035\n",
      "step: 364 loss: 5.387235164642334\n",
      "step: 365 loss: 5.678708553314209\n",
      "step: 366 loss: 5.134405136108398\n",
      "step: 367 loss: 5.39632511138916\n",
      "step: 368 loss: 5.490130424499512\n",
      "step: 369 loss: 5.573093414306641\n",
      "step: 370 loss: 5.31815242767334\n",
      "step: 371 loss: 5.745721340179443\n",
      "step: 372 loss: 5.668454170227051\n",
      "step: 373 loss: 5.470587253570557\n",
      "step: 374 loss: 5.615422248840332\n",
      "step: 375 loss: 5.597748279571533\n",
      "step: 376 loss: 5.440837383270264\n",
      "step: 377 loss: 5.137747764587402\n",
      "step: 378 loss: 5.473849773406982\n",
      "step: 379 loss: 5.547077178955078\n",
      "step: 380 loss: 5.442745685577393\n",
      "step: 381 loss: 5.2514567375183105\n",
      "step: 382 loss: 5.5538763999938965\n",
      "step: 383 loss: 5.462132930755615\n",
      "step: 384 loss: 5.492926597595215\n",
      "step: 385 loss: 5.506324768066406\n",
      "step: 386 loss: 5.66074275970459\n",
      "step: 387 loss: 5.612912654876709\n",
      "step: 388 loss: 5.493199825286865\n",
      "step: 389 loss: 5.666496753692627\n",
      "step: 390 loss: 5.349095344543457\n",
      "step: 391 loss: 5.465455055236816\n",
      "step: 392 loss: 5.631072998046875\n",
      "step: 393 loss: 5.5129570960998535\n",
      "step: 394 loss: 5.752129554748535\n",
      "step: 395 loss: 5.484541893005371\n",
      "step: 396 loss: 5.581249713897705\n",
      "step: 397 loss: 5.471187114715576\n",
      "step: 398 loss: 5.656166076660156\n",
      "step: 399 loss: 5.236672878265381\n",
      "step: 400 loss: 5.619555473327637\n",
      "step: 401 loss: 5.362863063812256\n",
      "step: 402 loss: 5.623040199279785\n",
      "step: 403 loss: 5.399107933044434\n",
      "step: 404 loss: 5.802779197692871\n",
      "step: 405 loss: 5.512975692749023\n",
      "step: 406 loss: 5.603744983673096\n",
      "step: 407 loss: 5.428379058837891\n",
      "step: 408 loss: 5.411320209503174\n",
      "step: 409 loss: 5.261688232421875\n",
      "step: 410 loss: 5.614852428436279\n",
      "step: 411 loss: 5.467457294464111\n",
      "step: 412 loss: 5.463829517364502\n",
      "step: 413 loss: 5.494980335235596\n",
      "step: 414 loss: 5.554850101470947\n",
      "step: 415 loss: 5.52091646194458\n",
      "step: 416 loss: 5.430660247802734\n",
      "step: 417 loss: 5.62565279006958\n",
      "step: 418 loss: 5.701394081115723\n",
      "step: 419 loss: 5.458436489105225\n",
      "step: 420 loss: 4.9709014892578125\n",
      "step: 421 loss: 5.2486066818237305\n",
      "step: 422 loss: 5.430046558380127\n",
      "step: 423 loss: 5.318317413330078\n",
      "step: 424 loss: 5.5011749267578125\n",
      "step: 425 loss: 5.249945163726807\n",
      "step: 426 loss: 5.238809585571289\n",
      "step: 427 loss: 5.438955307006836\n",
      "step: 428 loss: 5.546227931976318\n",
      "step: 429 loss: 5.143046855926514\n",
      "step: 430 loss: 5.376434803009033\n",
      "step: 431 loss: 5.380964279174805\n",
      "step: 432 loss: 5.504065990447998\n",
      "step: 433 loss: 5.201815605163574\n",
      "step: 434 loss: 5.372257232666016\n",
      "step: 435 loss: 5.253720283508301\n",
      "step: 436 loss: 5.550124168395996\n",
      "step: 437 loss: 5.548515796661377\n",
      "step: 438 loss: 5.3629841804504395\n",
      "step: 439 loss: 5.684279441833496\n",
      "step: 440 loss: 5.531954765319824\n",
      "step: 441 loss: 5.2971014976501465\n",
      "step: 442 loss: 5.637180328369141\n",
      "step: 443 loss: 5.4350361824035645\n",
      "step: 444 loss: 5.541426181793213\n",
      "step: 445 loss: 5.331271648406982\n",
      "step: 446 loss: 5.271076679229736\n",
      "step: 447 loss: 5.234900951385498\n",
      "step: 448 loss: 5.465338706970215\n",
      "step: 449 loss: 5.1890363693237305\n",
      "step: 450 loss: 5.501312255859375\n",
      "step: 451 loss: 5.545235633850098\n",
      "step: 452 loss: 5.394407272338867\n",
      "step: 453 loss: 5.5032057762146\n",
      "step: 454 loss: 5.3644208908081055\n",
      "step: 455 loss: 5.262118339538574\n",
      "step: 456 loss: 5.547231197357178\n",
      "step: 457 loss: 5.5091071128845215\n",
      "step: 458 loss: 5.705551624298096\n",
      "step: 459 loss: 5.435938835144043\n",
      "step: 460 loss: 5.309300899505615\n",
      "step: 461 loss: 5.1938090324401855\n",
      "step: 462 loss: 5.238770484924316\n",
      "step: 463 loss: 5.401351451873779\n",
      "step: 464 loss: 5.251980304718018\n",
      "step: 465 loss: 5.5109028816223145\n",
      "step: 466 loss: 5.500550270080566\n",
      "step: 467 loss: 5.2652106285095215\n",
      "step: 468 loss: 5.359733581542969\n",
      "step: 469 loss: 5.373655319213867\n",
      "step: 470 loss: 5.231747627258301\n",
      "step: 471 loss: 5.307844161987305\n",
      "step: 472 loss: 5.30057430267334\n",
      "step: 473 loss: 5.352930545806885\n",
      "step: 474 loss: 5.760789394378662\n",
      "step: 475 loss: 5.228109836578369\n",
      "step: 476 loss: 5.453149795532227\n",
      "step: 477 loss: 5.5498552322387695\n",
      "step: 478 loss: 5.415297985076904\n",
      "step: 479 loss: 5.713876724243164\n",
      "step: 480 loss: 5.38649320602417\n",
      "step: 481 loss: 5.59390926361084\n",
      "step: 482 loss: 5.619872093200684\n",
      "step: 483 loss: 5.371669769287109\n",
      "step: 484 loss: 5.530821323394775\n",
      "step: 485 loss: 5.410823345184326\n",
      "step: 486 loss: 5.401363372802734\n",
      "step: 487 loss: 5.507252216339111\n",
      "step: 488 loss: 5.662591934204102\n",
      "step: 489 loss: 5.504185199737549\n",
      "step: 490 loss: 5.38670539855957\n",
      "step: 491 loss: 5.110462665557861\n",
      "step: 492 loss: 5.483311653137207\n",
      "step: 493 loss: 5.3364691734313965\n",
      "step: 494 loss: 5.365664482116699\n",
      "step: 495 loss: 5.464175224304199\n",
      "step: 496 loss: 5.717071533203125\n",
      "step: 497 loss: 5.503141403198242\n",
      "step: 498 loss: 5.282035827636719\n",
      "step: 499 loss: 5.036139965057373\n",
      "step: 500 loss: 5.495208263397217\n",
      "step: 501 loss: 5.388850212097168\n",
      "step: 502 loss: 5.422210216522217\n",
      "step: 503 loss: 5.256342887878418\n",
      "step: 504 loss: 5.538547515869141\n",
      "step: 505 loss: 5.498495101928711\n",
      "step: 506 loss: 5.376420497894287\n",
      "step: 507 loss: 5.43043327331543\n",
      "step: 508 loss: 5.247141361236572\n",
      "step: 509 loss: 5.288018703460693\n",
      "step: 510 loss: 5.358653545379639\n",
      "step: 511 loss: 5.263772964477539\n",
      "step: 512 loss: 5.106363773345947\n",
      "step: 513 loss: 5.405531883239746\n",
      "step: 514 loss: 5.377597808837891\n",
      "step: 515 loss: 5.4050116539001465\n",
      "step: 516 loss: 5.174953460693359\n",
      "step: 517 loss: 5.381358623504639\n",
      "step: 518 loss: 5.322291851043701\n",
      "step: 519 loss: 5.165347576141357\n",
      "step: 520 loss: 5.591399669647217\n",
      "step: 521 loss: 5.512720584869385\n",
      "step: 522 loss: 5.354497909545898\n",
      "step: 523 loss: 5.41807222366333\n",
      "step: 524 loss: 5.236641883850098\n",
      "step: 525 loss: 5.196763038635254\n",
      "step: 526 loss: 5.291284084320068\n",
      "step: 527 loss: 5.2581586837768555\n",
      "step: 528 loss: 5.240658283233643\n",
      "step: 529 loss: 5.549230575561523\n",
      "step: 530 loss: 5.250273704528809\n",
      "step: 531 loss: 5.574863910675049\n",
      "step: 532 loss: 5.331819534301758\n",
      "step: 533 loss: 5.6120500564575195\n",
      "step: 534 loss: 5.48433256149292\n",
      "step: 535 loss: 5.533146858215332\n",
      "step: 536 loss: 5.637777328491211\n",
      "step: 537 loss: 5.325910568237305\n",
      "step: 538 loss: 5.1212592124938965\n",
      "step: 539 loss: 5.32412052154541\n",
      "step: 540 loss: 5.419698238372803\n",
      "step: 541 loss: 5.326165199279785\n",
      "step: 542 loss: 5.437130451202393\n",
      "step: 543 loss: 5.509174346923828\n",
      "step: 544 loss: 5.358105659484863\n",
      "step: 545 loss: 5.513093948364258\n",
      "step: 546 loss: 5.378763675689697\n",
      "step: 547 loss: 5.297789096832275\n",
      "step: 548 loss: 5.438133716583252\n",
      "step: 549 loss: 5.087650775909424\n",
      "step: 550 loss: 5.268523693084717\n",
      "step: 551 loss: 5.162841320037842\n",
      "step: 552 loss: 5.018127918243408\n",
      "step: 553 loss: 5.475365161895752\n",
      "step: 554 loss: 5.618714809417725\n",
      "step: 555 loss: 5.346137523651123\n",
      "step: 556 loss: 5.387688636779785\n",
      "step: 557 loss: 5.466218948364258\n",
      "step: 558 loss: 5.485786437988281\n",
      "step: 559 loss: 5.173928737640381\n",
      "step: 560 loss: 5.242422103881836\n",
      "step: 561 loss: 5.415837287902832\n",
      "step: 562 loss: 5.541929721832275\n",
      "step: 563 loss: 5.054957389831543\n",
      "step: 564 loss: 5.149306774139404\n",
      "step: 565 loss: 5.377326965332031\n",
      "step: 566 loss: 5.282100677490234\n",
      "step: 567 loss: 5.413172721862793\n",
      "step: 568 loss: 5.4170122146606445\n",
      "step: 569 loss: 5.252181053161621\n",
      "step: 570 loss: 5.1183390617370605\n",
      "step: 571 loss: 5.270631790161133\n",
      "step: 572 loss: 5.107900619506836\n",
      "step: 573 loss: 5.27559232711792\n",
      "step: 574 loss: 5.2249040603637695\n",
      "step: 575 loss: 5.407226085662842\n",
      "step: 576 loss: 5.845455646514893\n",
      "step: 577 loss: 5.135834217071533\n",
      "step: 578 loss: 5.533443927764893\n",
      "step: 579 loss: 5.259215831756592\n",
      "step: 580 loss: 5.316233158111572\n",
      "step: 581 loss: 5.222752094268799\n",
      "step: 582 loss: 5.178923606872559\n",
      "step: 583 loss: 5.269362449645996\n",
      "step: 584 loss: 5.190224647521973\n",
      "step: 585 loss: 5.427006244659424\n",
      "step: 586 loss: 5.343611717224121\n",
      "step: 587 loss: 5.374805450439453\n",
      "step: 588 loss: 5.115670204162598\n",
      "step: 589 loss: 5.314116954803467\n",
      "step: 590 loss: 4.921733856201172\n",
      "step: 591 loss: 5.500706672668457\n",
      "step: 592 loss: 4.971067428588867\n",
      "step: 593 loss: 5.2265801429748535\n",
      "step: 594 loss: 5.255181789398193\n",
      "step: 595 loss: 5.3333845138549805\n",
      "step: 596 loss: 5.249220848083496\n",
      "step: 597 loss: 5.361601829528809\n",
      "step: 598 loss: 5.306820869445801\n",
      "step: 599 loss: 5.366435527801514\n",
      "step: 600 loss: 5.135879039764404\n",
      "step: 601 loss: 5.05573034286499\n",
      "step: 602 loss: 5.287625789642334\n",
      "step: 603 loss: 5.229055881500244\n",
      "step: 604 loss: 5.597405910491943\n",
      "step: 605 loss: 5.391697406768799\n",
      "step: 606 loss: 5.3987932205200195\n",
      "step: 607 loss: 5.498779296875\n",
      "step: 608 loss: 5.081239223480225\n",
      "step: 609 loss: 5.147400379180908\n",
      "step: 610 loss: 5.5005035400390625\n",
      "step: 611 loss: 5.318078517913818\n",
      "step: 612 loss: 5.214507102966309\n",
      "step: 613 loss: 5.350399494171143\n",
      "step: 614 loss: 5.230026721954346\n",
      "step: 615 loss: 5.190133094787598\n",
      "step: 616 loss: 4.872791290283203\n",
      "step: 617 loss: 5.142773628234863\n",
      "step: 618 loss: 5.373189926147461\n",
      "step: 619 loss: 5.373979091644287\n",
      "step: 620 loss: 5.269786357879639\n",
      "step: 621 loss: 5.349419116973877\n",
      "step: 622 loss: 5.315966606140137\n",
      "step: 623 loss: 5.4230852127075195\n",
      "step: 624 loss: 5.281839370727539\n",
      "step: 625 loss: 5.075671195983887\n",
      "step: 626 loss: 4.794869422912598\n",
      "step: 627 loss: 5.360586643218994\n",
      "step: 628 loss: 5.174232482910156\n",
      "step: 629 loss: 5.256324291229248\n",
      "step: 630 loss: 5.2148261070251465\n",
      "step: 631 loss: 4.9616804122924805\n",
      "step: 632 loss: 5.0580644607543945\n",
      "step: 633 loss: 5.115654945373535\n",
      "step: 634 loss: 4.851748466491699\n",
      "step: 635 loss: 5.212813377380371\n",
      "step: 636 loss: 5.25266695022583\n",
      "step: 637 loss: 5.350165367126465\n",
      "step: 638 loss: 5.130197525024414\n",
      "step: 639 loss: 5.375075340270996\n",
      "step: 640 loss: 5.133171081542969\n",
      "step: 641 loss: 5.354068279266357\n",
      "step: 642 loss: 4.962900638580322\n",
      "step: 643 loss: 5.213905334472656\n",
      "step: 644 loss: 5.104561805725098\n",
      "step: 645 loss: 5.232815742492676\n",
      "step: 646 loss: 5.324724197387695\n",
      "step: 647 loss: 5.118915557861328\n",
      "step: 648 loss: 5.168444633483887\n",
      "step: 649 loss: 5.4669013023376465\n",
      "step: 650 loss: 5.382964611053467\n",
      "step: 651 loss: 5.156962871551514\n",
      "step: 652 loss: 5.173034191131592\n",
      "step: 653 loss: 5.573676586151123\n",
      "step: 654 loss: 5.202975749969482\n",
      "step: 655 loss: 5.2394328117370605\n",
      "step: 656 loss: 5.131614685058594\n",
      "step: 657 loss: 5.295668601989746\n",
      "step: 658 loss: 5.058467388153076\n",
      "step: 659 loss: 5.080822944641113\n",
      "step: 660 loss: 5.070185661315918\n",
      "step: 661 loss: 5.177980899810791\n",
      "step: 662 loss: 5.2081098556518555\n",
      "step: 663 loss: 5.406385898590088\n",
      "step: 664 loss: 5.199930667877197\n",
      "step: 665 loss: 5.194842338562012\n",
      "step: 666 loss: 5.089792728424072\n",
      "step: 667 loss: 5.110771179199219\n",
      "step: 668 loss: 5.1132426261901855\n",
      "step: 669 loss: 5.274895668029785\n",
      "step: 670 loss: 5.240344047546387\n",
      "step: 671 loss: 5.090541362762451\n",
      "step: 672 loss: 5.024474143981934\n",
      "step: 673 loss: 4.715468883514404\n",
      "step: 674 loss: 5.026448726654053\n",
      "step: 675 loss: 5.088054656982422\n",
      "step: 676 loss: 5.012874603271484\n",
      "step: 677 loss: 5.221280097961426\n",
      "step: 678 loss: 5.116856098175049\n",
      "step: 679 loss: 4.849504470825195\n",
      "step: 680 loss: 4.918813228607178\n",
      "step: 681 loss: 5.034749984741211\n",
      "step: 682 loss: 5.049553871154785\n",
      "step: 683 loss: 5.102625370025635\n",
      "step: 684 loss: 5.141748905181885\n",
      "step: 685 loss: 5.269575119018555\n",
      "step: 686 loss: 5.471871376037598\n",
      "step: 687 loss: 5.098211765289307\n",
      "step: 688 loss: 5.2712788581848145\n",
      "step: 689 loss: 5.261043548583984\n",
      "step: 690 loss: 5.389780521392822\n",
      "step: 691 loss: 5.274636268615723\n",
      "step: 692 loss: 5.14543342590332\n",
      "step: 693 loss: 5.34398889541626\n",
      "step: 694 loss: 5.19149923324585\n",
      "step: 695 loss: 5.15847635269165\n",
      "step: 696 loss: 5.42990779876709\n",
      "step: 697 loss: 5.23413610458374\n",
      "step: 698 loss: 5.25241756439209\n",
      "step: 699 loss: 5.435285568237305\n",
      "step: 700 loss: 5.311211585998535\n",
      "step: 701 loss: 5.063168048858643\n",
      "step: 702 loss: 5.395140171051025\n",
      "step: 703 loss: 5.332468509674072\n",
      "step: 704 loss: 4.953248023986816\n",
      "step: 705 loss: 4.948056221008301\n",
      "step: 706 loss: 4.977576732635498\n",
      "step: 707 loss: 5.59220552444458\n",
      "step: 708 loss: 5.206108570098877\n",
      "step: 709 loss: 5.384307861328125\n",
      "step: 710 loss: 5.208913803100586\n",
      "step: 711 loss: 5.2871928215026855\n",
      "step: 712 loss: 5.124079704284668\n",
      "step: 713 loss: 5.2222819328308105\n",
      "step: 714 loss: 5.1676025390625\n",
      "step: 715 loss: 5.231095314025879\n",
      "step: 716 loss: 5.104361057281494\n",
      "step: 717 loss: 5.51369047164917\n",
      "step: 718 loss: 5.407883167266846\n",
      "step: 719 loss: 5.046807765960693\n",
      "step: 720 loss: 5.354328632354736\n",
      "step: 721 loss: 5.279993534088135\n",
      "step: 722 loss: 5.318448066711426\n",
      "step: 723 loss: 5.331686496734619\n",
      "step: 724 loss: 5.352712631225586\n",
      "step: 725 loss: 5.193758964538574\n",
      "step: 726 loss: 5.406552791595459\n",
      "step: 727 loss: 5.238065719604492\n",
      "step: 728 loss: 5.7057671546936035\n",
      "step: 729 loss: 5.42561674118042\n",
      "step: 730 loss: 5.672242641448975\n",
      "step: 731 loss: 5.204314708709717\n",
      "step: 732 loss: 5.241394519805908\n",
      "step: 733 loss: 5.5090131759643555\n",
      "step: 734 loss: 5.1146240234375\n",
      "step: 735 loss: 5.231447696685791\n",
      "step: 736 loss: 4.96746301651001\n",
      "step: 737 loss: 5.064675331115723\n",
      "step: 738 loss: 5.381467819213867\n",
      "step: 739 loss: 5.521542549133301\n",
      "step: 740 loss: 5.347189903259277\n",
      "step: 741 loss: 5.400819778442383\n",
      "step: 742 loss: 5.448780536651611\n",
      "step: 743 loss: 5.229587554931641\n",
      "step: 744 loss: 5.189716815948486\n",
      "step: 745 loss: 5.250061511993408\n",
      "step: 746 loss: 5.237899303436279\n",
      "step: 747 loss: 5.414782524108887\n",
      "step: 748 loss: 5.107532501220703\n",
      "step: 749 loss: 5.1200666427612305\n",
      "step: 750 loss: 5.154169082641602\n",
      "step: 751 loss: 5.0193772315979\n",
      "step: 752 loss: 5.355424880981445\n",
      "step: 753 loss: 5.048627853393555\n",
      "step: 754 loss: 5.398677825927734\n",
      "step: 755 loss: 5.356941223144531\n",
      "step: 756 loss: 5.314809322357178\n",
      "step: 757 loss: 5.453244686126709\n",
      "step: 758 loss: 5.188501358032227\n",
      "step: 759 loss: 5.394913196563721\n",
      "step: 760 loss: 5.249104022979736\n",
      "step: 761 loss: 5.075662612915039\n",
      "step: 762 loss: 5.104028224945068\n",
      "step: 763 loss: 5.388229846954346\n",
      "step: 764 loss: 5.216512203216553\n",
      "step: 765 loss: 5.202264308929443\n",
      "step: 766 loss: 5.0969414710998535\n",
      "step: 767 loss: 5.502422332763672\n",
      "step: 768 loss: 4.918415546417236\n",
      "step: 769 loss: 5.235143184661865\n",
      "step: 770 loss: 5.501377582550049\n",
      "step: 771 loss: 5.324389457702637\n",
      "step: 772 loss: 5.231131076812744\n",
      "step: 773 loss: 4.941807746887207\n",
      "step: 774 loss: 5.3644585609436035\n",
      "step: 775 loss: 5.188202381134033\n",
      "step: 776 loss: 5.3753275871276855\n",
      "step: 777 loss: 5.273033618927002\n",
      "step: 778 loss: 5.362183094024658\n",
      "step: 779 loss: 5.267100811004639\n",
      "step: 780 loss: 5.4866485595703125\n",
      "step: 781 loss: 5.633965015411377\n",
      "step: 782 loss: 5.2034783363342285\n",
      "step: 783 loss: 5.230566024780273\n",
      "step: 784 loss: 5.17161226272583\n",
      "step: 785 loss: 5.043291091918945\n",
      "step: 786 loss: 5.178758144378662\n",
      "step: 787 loss: 5.329235553741455\n",
      "step: 788 loss: 5.192372798919678\n",
      "step: 789 loss: 5.202205657958984\n",
      "step: 790 loss: 5.26765251159668\n",
      "step: 791 loss: 5.1944756507873535\n",
      "step: 792 loss: 5.499921798706055\n",
      "step: 793 loss: 5.431431293487549\n",
      "step: 794 loss: 5.020459175109863\n",
      "step: 795 loss: 5.521208763122559\n",
      "step: 796 loss: 5.185995101928711\n",
      "step: 797 loss: 5.2313313484191895\n",
      "step: 798 loss: 5.120377540588379\n",
      "step: 799 loss: 5.2019124031066895\n",
      "step: 800 loss: 5.303571701049805\n",
      "step: 801 loss: 5.220471382141113\n",
      "step: 802 loss: 5.154151439666748\n",
      "step: 803 loss: 5.171526908874512\n",
      "step: 804 loss: 5.351556777954102\n",
      "step: 805 loss: 5.50199556350708\n",
      "step: 806 loss: 4.799357891082764\n",
      "step: 807 loss: 5.08615779876709\n",
      "step: 808 loss: 5.147337913513184\n",
      "step: 809 loss: 5.426391124725342\n",
      "step: 810 loss: 5.150540351867676\n",
      "step: 811 loss: 5.0490593910217285\n",
      "step: 812 loss: 5.162633419036865\n",
      "step: 813 loss: 5.216358661651611\n",
      "step: 814 loss: 5.138036251068115\n",
      "step: 815 loss: 5.245285511016846\n",
      "step: 816 loss: 5.3233232498168945\n",
      "step: 817 loss: 5.089083671569824\n",
      "step: 818 loss: 5.146072864532471\n",
      "step: 819 loss: 5.104915142059326\n",
      "step: 820 loss: 5.392361164093018\n",
      "step: 821 loss: 5.217649936676025\n",
      "step: 822 loss: 5.011185169219971\n",
      "step: 823 loss: 5.225376129150391\n",
      "step: 824 loss: 5.026794910430908\n",
      "step: 825 loss: 5.105402946472168\n",
      "step: 826 loss: 5.071195602416992\n",
      "step: 827 loss: 5.440927505493164\n",
      "step: 828 loss: 5.25933313369751\n",
      "step: 829 loss: 4.835172653198242\n",
      "step: 830 loss: 5.164793491363525\n",
      "step: 831 loss: 5.024761199951172\n",
      "step: 832 loss: 5.210517883300781\n",
      "step: 833 loss: 5.009721279144287\n",
      "step: 834 loss: 5.07940149307251\n",
      "step: 835 loss: 5.041998386383057\n",
      "step: 836 loss: 5.080616474151611\n",
      "step: 837 loss: 4.9839372634887695\n",
      "step: 838 loss: 5.258243083953857\n",
      "step: 839 loss: 5.108450412750244\n",
      "step: 840 loss: 5.172843933105469\n",
      "step: 841 loss: 5.212704181671143\n",
      "step: 842 loss: 5.226871967315674\n",
      "step: 843 loss: 5.129079818725586\n",
      "step: 844 loss: 5.084506034851074\n",
      "step: 845 loss: 5.118915557861328\n",
      "step: 846 loss: 5.298954010009766\n",
      "step: 847 loss: 5.233994483947754\n",
      "step: 848 loss: 5.0854644775390625\n",
      "step: 849 loss: 5.371612548828125\n",
      "step: 850 loss: 4.973560333251953\n",
      "step: 851 loss: 5.143144130706787\n",
      "step: 852 loss: 5.149519443511963\n",
      "step: 853 loss: 4.945808410644531\n",
      "step: 854 loss: 5.1864776611328125\n",
      "step: 855 loss: 5.015719413757324\n",
      "step: 856 loss: 5.192933559417725\n",
      "step: 857 loss: 5.220004081726074\n",
      "step: 858 loss: 5.271978855133057\n",
      "step: 859 loss: 5.083526134490967\n",
      "step: 860 loss: 4.980327129364014\n",
      "step: 861 loss: 5.022103786468506\n",
      "step: 862 loss: 5.1858391761779785\n",
      "step: 863 loss: 5.082267761230469\n",
      "step: 864 loss: 5.47991943359375\n",
      "step: 865 loss: 5.43991756439209\n",
      "step: 866 loss: 5.283214092254639\n",
      "step: 867 loss: 5.193202018737793\n",
      "step: 868 loss: 5.107064247131348\n",
      "step: 869 loss: 5.225931644439697\n",
      "step: 870 loss: 5.084755897521973\n",
      "step: 871 loss: 5.241467475891113\n",
      "step: 872 loss: 5.095907211303711\n",
      "step: 873 loss: 5.407286643981934\n",
      "step: 874 loss: 5.127538681030273\n",
      "step: 875 loss: 5.119969844818115\n",
      "step: 876 loss: 5.194766998291016\n",
      "step: 877 loss: 5.359451770782471\n",
      "step: 878 loss: 5.328362941741943\n",
      "step: 879 loss: 5.28582763671875\n",
      "step: 880 loss: 5.271351337432861\n",
      "step: 881 loss: 5.07505464553833\n",
      "step: 882 loss: 5.528214454650879\n",
      "step: 883 loss: 5.3820013999938965\n",
      "step: 884 loss: 5.122381210327148\n",
      "step: 885 loss: 5.2151689529418945\n",
      "step: 886 loss: 5.112269878387451\n",
      "step: 887 loss: 5.14227294921875\n",
      "step: 888 loss: 4.787781715393066\n",
      "step: 889 loss: 5.1576032638549805\n",
      "step: 890 loss: 5.324357032775879\n",
      "step: 891 loss: 5.0901994705200195\n",
      "step: 892 loss: 4.984217166900635\n",
      "step: 893 loss: 5.154543876647949\n",
      "step: 894 loss: 5.266891956329346\n",
      "step: 895 loss: 5.336918354034424\n",
      "step: 896 loss: 5.033114433288574\n",
      "step: 897 loss: 5.516454696655273\n",
      "step: 898 loss: 5.054935455322266\n",
      "step: 899 loss: 5.526816368103027\n",
      "step: 900 loss: 5.318116664886475\n",
      "step: 901 loss: 5.408874034881592\n",
      "step: 902 loss: 5.274760723114014\n",
      "step: 903 loss: 5.443355560302734\n",
      "step: 904 loss: 5.229727745056152\n",
      "step: 905 loss: 4.974490165710449\n",
      "step: 906 loss: 5.142950057983398\n",
      "step: 907 loss: 4.984315395355225\n",
      "step: 908 loss: 5.148061752319336\n",
      "step: 909 loss: 5.041488170623779\n",
      "step: 910 loss: 5.222590923309326\n",
      "step: 911 loss: 5.169908046722412\n",
      "step: 912 loss: 4.984130859375\n",
      "step: 913 loss: 5.191906929016113\n",
      "step: 914 loss: 5.135678291320801\n",
      "step: 915 loss: 4.9660515785217285\n",
      "step: 916 loss: 4.962388038635254\n",
      "step: 917 loss: 5.391167163848877\n",
      "step: 918 loss: 5.192770481109619\n",
      "step: 919 loss: 4.8246965408325195\n",
      "step: 920 loss: 5.032077789306641\n",
      "step: 921 loss: 4.7232160568237305\n",
      "step: 922 loss: 5.15134334564209\n",
      "step: 923 loss: 5.262596130371094\n",
      "step: 924 loss: 5.133564472198486\n",
      "step: 925 loss: 5.149000644683838\n",
      "step: 926 loss: 4.871519565582275\n",
      "step: 927 loss: 5.088592529296875\n",
      "step: 928 loss: 5.120038032531738\n",
      "step: 929 loss: 5.140331745147705\n",
      "step: 930 loss: 5.466920375823975\n",
      "step: 931 loss: 4.942044258117676\n",
      "step: 932 loss: 5.56427526473999\n",
      "step: 933 loss: 5.2664794921875\n",
      "step: 934 loss: 5.1516900062561035\n",
      "step: 935 loss: 5.284134387969971\n",
      "step: 936 loss: 4.913142204284668\n",
      "step: 937 loss: 5.0731048583984375\n",
      "step: 938 loss: 5.165505886077881\n",
      "step: 939 loss: 5.257022857666016\n",
      "step: 940 loss: 4.992293834686279\n",
      "step: 941 loss: 4.85862922668457\n",
      "step: 942 loss: 4.949419975280762\n",
      "step: 943 loss: 5.25154972076416\n",
      "step: 944 loss: 4.956594467163086\n",
      "step: 945 loss: 5.270809650421143\n",
      "step: 946 loss: 5.0956854820251465\n",
      "step: 947 loss: 4.9558281898498535\n",
      "step: 948 loss: 5.006011486053467\n",
      "step: 949 loss: 4.989717960357666\n",
      "step: 950 loss: 5.211409091949463\n",
      "step: 951 loss: 4.915322303771973\n",
      "step: 952 loss: 5.28600549697876\n",
      "step: 953 loss: 4.853906154632568\n",
      "step: 954 loss: 5.050466060638428\n",
      "step: 955 loss: 5.261383056640625\n",
      "step: 956 loss: 5.223948001861572\n",
      "step: 957 loss: 5.238975524902344\n",
      "step: 958 loss: 4.964877605438232\n",
      "step: 959 loss: 5.1028571128845215\n",
      "step: 960 loss: 4.984098434448242\n",
      "step: 961 loss: 4.841403007507324\n",
      "step: 962 loss: 4.956967353820801\n",
      "step: 963 loss: 5.14810037612915\n",
      "step: 964 loss: 4.789238929748535\n",
      "step: 965 loss: 5.184097766876221\n",
      "step: 966 loss: 5.194032669067383\n",
      "step: 967 loss: 5.084967136383057\n",
      "step: 968 loss: 4.846580982208252\n",
      "step: 969 loss: 5.097928524017334\n",
      "step: 970 loss: 5.373466491699219\n",
      "step: 971 loss: 4.935074329376221\n",
      "step: 972 loss: 5.198935031890869\n",
      "step: 973 loss: 4.72503662109375\n",
      "step: 974 loss: 5.11533260345459\n",
      "step: 975 loss: 4.960746765136719\n",
      "step: 976 loss: 5.111773490905762\n",
      "step: 977 loss: 5.1046833992004395\n",
      "step: 978 loss: 5.142083168029785\n",
      "step: 979 loss: 5.3738532066345215\n",
      "step: 980 loss: 5.073556900024414\n",
      "step: 981 loss: 5.058713436126709\n",
      "step: 982 loss: 4.9196672439575195\n",
      "step: 983 loss: 4.933441638946533\n",
      "step: 984 loss: 5.111784934997559\n",
      "step: 985 loss: 5.1330766677856445\n",
      "step: 986 loss: 4.95352840423584\n",
      "step: 987 loss: 4.773754119873047\n",
      "step: 988 loss: 5.214953422546387\n",
      "step: 989 loss: 4.952165603637695\n",
      "step: 990 loss: 4.832542896270752\n",
      "step: 991 loss: 5.05959415435791\n",
      "step: 992 loss: 5.1039042472839355\n",
      "step: 993 loss: 5.162894248962402\n",
      "step: 994 loss: 5.006378173828125\n",
      "step: 995 loss: 4.945923805236816\n",
      "step: 996 loss: 5.145184516906738\n",
      "step: 997 loss: 5.303912162780762\n",
      "step: 998 loss: 5.066422462463379\n",
      "step: 999 loss: 5.008967876434326\n",
      "step: 1000 loss: 4.938110828399658\n",
      "step: 1001 loss: 5.189006805419922\n",
      "step: 1002 loss: 5.282321929931641\n",
      "step: 1003 loss: 4.919882774353027\n",
      "step: 1004 loss: 4.847965717315674\n",
      "step: 1005 loss: 5.131604194641113\n",
      "step: 1006 loss: 4.908189296722412\n",
      "step: 1007 loss: 4.755765438079834\n",
      "step: 1008 loss: 5.006070613861084\n",
      "step: 1009 loss: 4.739037036895752\n",
      "step: 1010 loss: 5.026826858520508\n",
      "step: 1011 loss: 5.042699813842773\n",
      "step: 1012 loss: 4.90318489074707\n",
      "step: 1013 loss: 5.03251314163208\n",
      "step: 1014 loss: 4.886158466339111\n",
      "step: 1015 loss: 5.017284393310547\n",
      "step: 1016 loss: 5.093963623046875\n",
      "step: 1017 loss: 5.268560886383057\n",
      "step: 1018 loss: 4.932953834533691\n",
      "step: 1019 loss: 4.757298946380615\n",
      "step: 1020 loss: 5.050492763519287\n",
      "step: 1021 loss: 5.161881923675537\n",
      "step: 1022 loss: 4.695499897003174\n",
      "step: 1023 loss: 5.073143005371094\n",
      "step: 1024 loss: 5.019745826721191\n",
      "step: 1025 loss: 4.989758014678955\n",
      "step: 1026 loss: 5.017714977264404\n",
      "step: 1027 loss: 4.890462398529053\n",
      "step: 1028 loss: 5.004692077636719\n",
      "step: 1029 loss: 5.190074920654297\n",
      "step: 1030 loss: 5.212831020355225\n",
      "step: 1031 loss: 5.370681285858154\n",
      "step: 1032 loss: 4.84718656539917\n",
      "step: 1033 loss: 4.908447742462158\n",
      "step: 1034 loss: 4.994085311889648\n",
      "step: 1035 loss: 4.654618263244629\n",
      "step: 1036 loss: 4.946267604827881\n",
      "step: 1037 loss: 4.759644508361816\n",
      "step: 1038 loss: 4.870724201202393\n",
      "step: 1039 loss: 5.129151344299316\n",
      "step: 1040 loss: 5.190832138061523\n",
      "step: 1041 loss: 4.854415416717529\n",
      "step: 1042 loss: 5.331900596618652\n",
      "step: 1043 loss: 5.0857133865356445\n",
      "step: 1044 loss: 5.141168594360352\n",
      "step: 1045 loss: 5.040939807891846\n",
      "step: 1046 loss: 5.264540672302246\n",
      "step: 1047 loss: 5.031807899475098\n",
      "step: 1048 loss: 5.144992828369141\n",
      "step: 1049 loss: 5.2209625244140625\n",
      "step: 1050 loss: 4.932131767272949\n",
      "step: 1051 loss: 5.138097286224365\n",
      "step: 1052 loss: 5.183854103088379\n",
      "step: 1053 loss: 5.246209621429443\n",
      "step: 1054 loss: 5.01903772354126\n",
      "step: 1055 loss: 5.071943283081055\n",
      "step: 1056 loss: 5.002779483795166\n",
      "step: 1057 loss: 5.278292179107666\n",
      "step: 1058 loss: 5.355975151062012\n",
      "step: 1059 loss: 5.077736854553223\n",
      "step: 1060 loss: 5.041441440582275\n",
      "step: 1061 loss: 4.943789958953857\n",
      "step: 1062 loss: 5.035393238067627\n",
      "step: 1063 loss: 5.114077091217041\n",
      "step: 1064 loss: 5.0490803718566895\n",
      "step: 1065 loss: 5.212054252624512\n",
      "step: 1066 loss: 5.000238418579102\n",
      "step: 1067 loss: 4.861692428588867\n",
      "step: 1068 loss: 4.9721527099609375\n",
      "step: 1069 loss: 5.1415791511535645\n",
      "step: 1070 loss: 5.315574645996094\n",
      "step: 1071 loss: 5.1253743171691895\n",
      "step: 1072 loss: 5.014176368713379\n",
      "step: 1073 loss: 5.140989780426025\n",
      "step: 1074 loss: 5.315450191497803\n",
      "step: 1075 loss: 5.092982769012451\n",
      "step: 1076 loss: 5.122264385223389\n",
      "step: 1077 loss: 5.100519180297852\n",
      "step: 1078 loss: 4.842452526092529\n",
      "step: 1079 loss: 5.215193748474121\n",
      "step: 1080 loss: 5.152862071990967\n",
      "step: 1081 loss: 5.258586883544922\n",
      "step: 1082 loss: 5.077260494232178\n",
      "step: 1083 loss: 5.237217903137207\n",
      "step: 1084 loss: 5.118298053741455\n",
      "step: 1085 loss: 5.105301856994629\n",
      "step: 1086 loss: 5.060429573059082\n",
      "step: 1087 loss: 5.040926933288574\n",
      "step: 1088 loss: 4.901179313659668\n",
      "step: 1089 loss: 5.129246234893799\n",
      "step: 1090 loss: 5.043277263641357\n",
      "step: 1091 loss: 4.770761489868164\n",
      "step: 1092 loss: 4.950474739074707\n",
      "step: 1093 loss: 4.923762321472168\n",
      "step: 1094 loss: 5.231533050537109\n",
      "step: 1095 loss: 5.039910316467285\n",
      "step: 1096 loss: 5.107202529907227\n",
      "step: 1097 loss: 4.975673675537109\n",
      "step: 1098 loss: 5.043395519256592\n",
      "step: 1099 loss: 5.046718597412109\n",
      "step: 1100 loss: 5.0515289306640625\n",
      "step: 1101 loss: 4.894701957702637\n",
      "step: 1102 loss: 4.989434719085693\n",
      "step: 1103 loss: 4.757453441619873\n",
      "step: 1104 loss: 5.183950424194336\n",
      "step: 1105 loss: 5.2605719566345215\n",
      "step: 1106 loss: 5.056155204772949\n",
      "step: 1107 loss: 5.314444541931152\n",
      "step: 1108 loss: 5.204403400421143\n",
      "step: 1109 loss: 5.254158973693848\n",
      "step: 1110 loss: 5.040611743927002\n",
      "step: 1111 loss: 5.031278610229492\n",
      "step: 1112 loss: 4.859556198120117\n",
      "step: 1113 loss: 5.228732585906982\n",
      "step: 1114 loss: 4.861730575561523\n",
      "step: 1115 loss: 5.187773704528809\n",
      "step: 1116 loss: 5.093523025512695\n",
      "step: 1117 loss: 4.890212535858154\n",
      "step: 1118 loss: 5.062369346618652\n",
      "step: 1119 loss: 5.032756805419922\n",
      "step: 1120 loss: 4.971592903137207\n",
      "step: 1121 loss: 4.832752704620361\n",
      "step: 1122 loss: 4.955240726470947\n",
      "step: 1123 loss: 5.066787242889404\n",
      "step: 1124 loss: 5.096346378326416\n",
      "step: 1125 loss: 5.263576030731201\n",
      "step: 1126 loss: 4.841095924377441\n",
      "step: 1127 loss: 4.968607425689697\n",
      "step: 1128 loss: 4.8678083419799805\n",
      "step: 1129 loss: 5.138877868652344\n",
      "step: 1130 loss: 5.060880661010742\n",
      "step: 1131 loss: 5.1840667724609375\n",
      "step: 1132 loss: 5.353772163391113\n",
      "step: 1133 loss: 4.865025997161865\n",
      "step: 1134 loss: 4.89724063873291\n",
      "step: 1135 loss: 4.929182052612305\n",
      "step: 1136 loss: 5.21640157699585\n",
      "step: 1137 loss: 4.875633716583252\n",
      "step: 1138 loss: 4.986755847930908\n",
      "step: 1139 loss: 4.996660232543945\n",
      "step: 1140 loss: 4.89445161819458\n",
      "step: 1141 loss: 4.916712284088135\n",
      "step: 1142 loss: 5.1167144775390625\n",
      "step: 1143 loss: 4.944967269897461\n",
      "step: 1144 loss: 5.2257866859436035\n",
      "step: 1145 loss: 5.003681182861328\n",
      "step: 1146 loss: 5.107333183288574\n",
      "step: 1147 loss: 4.904811382293701\n",
      "step: 1148 loss: 5.090635776519775\n",
      "step: 1149 loss: 4.991255760192871\n",
      "step: 1150 loss: 5.122875690460205\n",
      "step: 1151 loss: 4.89926290512085\n",
      "step: 1152 loss: 5.259213924407959\n",
      "step: 1153 loss: 4.975912094116211\n",
      "step: 1154 loss: 4.922584533691406\n",
      "step: 1155 loss: 5.133918762207031\n",
      "step: 1156 loss: 5.1229729652404785\n",
      "step: 1157 loss: 5.190433979034424\n",
      "step: 1158 loss: 4.932738780975342\n",
      "step: 1159 loss: 5.23209285736084\n",
      "step: 1160 loss: 4.710270404815674\n",
      "step: 1161 loss: 5.281304359436035\n",
      "step: 1162 loss: 5.238855838775635\n",
      "step: 1163 loss: 5.307459354400635\n",
      "step: 1164 loss: 4.929763317108154\n",
      "step: 1165 loss: 5.15802001953125\n",
      "step: 1166 loss: 4.86682653427124\n",
      "step: 1167 loss: 5.13786506652832\n",
      "step: 1168 loss: 4.963778018951416\n",
      "step: 1169 loss: 4.864687919616699\n",
      "step: 1170 loss: 5.278799057006836\n",
      "step: 1171 loss: 4.818365097045898\n",
      "step: 1172 loss: 5.094053745269775\n",
      "step: 1173 loss: 4.944550514221191\n",
      "step: 1174 loss: 5.0506744384765625\n",
      "step: 1175 loss: 4.7130632400512695\n",
      "step: 1176 loss: 5.148144721984863\n",
      "step: 1177 loss: 4.95201301574707\n",
      "step: 1178 loss: 5.040814399719238\n",
      "step: 1179 loss: 5.026986122131348\n",
      "step: 1180 loss: 5.408043384552002\n",
      "step: 1181 loss: 5.050351142883301\n",
      "step: 1182 loss: 5.1102399826049805\n",
      "step: 1183 loss: 5.017510890960693\n",
      "step: 1184 loss: 5.142508029937744\n",
      "step: 1185 loss: 5.0471391677856445\n",
      "step: 1186 loss: 4.796274185180664\n",
      "step: 1187 loss: 5.168933868408203\n",
      "step: 1188 loss: 5.076446533203125\n",
      "step: 1189 loss: 4.935199737548828\n",
      "step: 1190 loss: 5.069167613983154\n",
      "step: 1191 loss: 4.774690628051758\n",
      "step: 1192 loss: 4.915822505950928\n",
      "step: 1193 loss: 5.169516563415527\n",
      "step: 1194 loss: 5.106657981872559\n",
      "step: 1195 loss: 4.74822998046875\n",
      "step: 1196 loss: 5.260311603546143\n",
      "step: 1197 loss: 4.900459289550781\n",
      "step: 1198 loss: 4.906078815460205\n",
      "step: 1199 loss: 4.849693298339844\n",
      "step: 1200 loss: 4.6927170753479\n",
      "step: 1201 loss: 4.9679856300354\n",
      "step: 1202 loss: 4.762139320373535\n",
      "step: 1203 loss: 4.859843730926514\n",
      "step: 1204 loss: 4.714280605316162\n",
      "step: 1205 loss: 5.039037704467773\n",
      "step: 1206 loss: 4.9641337394714355\n",
      "step: 1207 loss: 5.191592693328857\n",
      "step: 1208 loss: 4.9976606369018555\n",
      "step: 1209 loss: 4.9794602394104\n",
      "step: 1210 loss: 5.206358909606934\n",
      "step: 1211 loss: 4.724257946014404\n",
      "step: 1212 loss: 4.640449523925781\n",
      "step: 1213 loss: 4.9763875007629395\n",
      "step: 1214 loss: 5.066722869873047\n",
      "step: 1215 loss: 4.647730350494385\n",
      "step: 1216 loss: 5.097235679626465\n",
      "step: 1217 loss: 5.028335094451904\n",
      "step: 1218 loss: 4.884418964385986\n",
      "step: 1219 loss: 5.215543270111084\n",
      "step: 1220 loss: 4.800069332122803\n",
      "step: 1221 loss: 4.944590091705322\n",
      "step: 1222 loss: 4.842360973358154\n",
      "step: 1223 loss: 4.9064154624938965\n",
      "step: 1224 loss: 5.297338008880615\n",
      "step: 1225 loss: 4.885754108428955\n",
      "step: 1226 loss: 4.673046588897705\n",
      "step: 1227 loss: 5.018144607543945\n",
      "step: 1228 loss: 4.730278491973877\n",
      "step: 1229 loss: 5.146294593811035\n",
      "step: 1230 loss: 4.883724212646484\n",
      "step: 1231 loss: 4.679900646209717\n",
      "step: 1232 loss: 4.799760341644287\n",
      "step: 1233 loss: 4.7558112144470215\n",
      "step: 1234 loss: 5.048996448516846\n",
      "step: 1235 loss: 4.987313270568848\n",
      "step: 1236 loss: 5.066995620727539\n",
      "step: 1237 loss: 5.031797409057617\n",
      "step: 1238 loss: 4.740082263946533\n",
      "step: 1239 loss: 4.744603157043457\n",
      "step: 1240 loss: 4.96139669418335\n",
      "step: 1241 loss: 5.029101371765137\n",
      "step: 1242 loss: 5.007959365844727\n",
      "step: 1243 loss: 5.078341007232666\n",
      "step: 1244 loss: 4.887265682220459\n",
      "step: 1245 loss: 4.913848400115967\n",
      "step: 1246 loss: 4.893921852111816\n",
      "step: 1247 loss: 5.079092025756836\n",
      "step: 1248 loss: 5.133956432342529\n",
      "step: 1249 loss: 4.795672416687012\n",
      "step: 1250 loss: 5.000907897949219\n",
      "step: 1251 loss: 5.2787346839904785\n",
      "step: 1252 loss: 4.958637237548828\n",
      "step: 1253 loss: 4.980400085449219\n",
      "step: 1254 loss: 5.215821743011475\n",
      "step: 1255 loss: 5.261782646179199\n",
      "step: 1256 loss: 5.153343677520752\n",
      "step: 1257 loss: 5.2779107093811035\n",
      "step: 1258 loss: 4.951842308044434\n",
      "step: 1259 loss: 4.811140537261963\n",
      "step: 1260 loss: 4.993710994720459\n",
      "step: 1261 loss: 4.999774932861328\n",
      "step: 1262 loss: 5.234160423278809\n",
      "step: 1263 loss: 4.939116954803467\n",
      "step: 1264 loss: 4.884782314300537\n",
      "step: 1265 loss: 5.065453052520752\n",
      "step: 1266 loss: 4.740607738494873\n",
      "step: 1267 loss: 5.016551494598389\n",
      "step: 1268 loss: 5.026800155639648\n",
      "step: 1269 loss: 5.038434028625488\n",
      "step: 1270 loss: 4.7150797843933105\n",
      "step: 1271 loss: 4.7998433113098145\n",
      "step: 1272 loss: 4.7212958335876465\n",
      "step: 1273 loss: 5.105226516723633\n",
      "step: 1274 loss: 4.912421226501465\n",
      "step: 1275 loss: 4.727497100830078\n",
      "step: 1276 loss: 5.011266231536865\n",
      "step: 1277 loss: 5.06956672668457\n",
      "step: 1278 loss: 4.8620381355285645\n",
      "step: 1279 loss: 4.855255126953125\n",
      "step: 1280 loss: 5.121418476104736\n",
      "step: 1281 loss: 4.986117839813232\n",
      "step: 1282 loss: 5.145927429199219\n",
      "step: 1283 loss: 5.313369274139404\n",
      "step: 1284 loss: 4.95090389251709\n",
      "step: 1285 loss: 5.008326053619385\n",
      "step: 1286 loss: 4.924768447875977\n",
      "step: 1287 loss: 5.220426559448242\n",
      "step: 1288 loss: 5.137134552001953\n",
      "step: 1289 loss: 4.890922546386719\n",
      "step: 1290 loss: 5.007462978363037\n",
      "step: 1291 loss: 4.9889349937438965\n",
      "step: 1292 loss: 4.9831223487854\n",
      "step: 1293 loss: 4.789729595184326\n",
      "step: 1294 loss: 4.984781742095947\n",
      "step: 1295 loss: 5.113188743591309\n",
      "step: 1296 loss: 4.965466022491455\n",
      "step: 1297 loss: 4.775931358337402\n",
      "step: 1298 loss: 5.1094136238098145\n",
      "step: 1299 loss: 4.955523490905762\n",
      "step: 1300 loss: 4.942737102508545\n",
      "step: 1301 loss: 4.818758487701416\n",
      "step: 1302 loss: 4.7996039390563965\n",
      "step: 1303 loss: 4.515749931335449\n",
      "step: 1304 loss: 4.9773969650268555\n",
      "step: 1305 loss: 5.331779479980469\n",
      "step: 1306 loss: 4.849303245544434\n",
      "step: 1307 loss: 5.1316328048706055\n",
      "step: 1308 loss: 4.9746527671813965\n",
      "step: 1309 loss: 4.852082252502441\n",
      "step: 1310 loss: 4.669574737548828\n",
      "step: 1311 loss: 4.70719051361084\n",
      "step: 1312 loss: 5.027800559997559\n",
      "step: 1313 loss: 5.175430774688721\n",
      "step: 1314 loss: 5.022881507873535\n",
      "step: 1315 loss: 5.088655471801758\n",
      "step: 1316 loss: 4.937950134277344\n",
      "step: 1317 loss: 4.785092353820801\n",
      "step: 1318 loss: 4.816163063049316\n",
      "step: 1319 loss: 5.126912593841553\n",
      "step: 1320 loss: 5.034707069396973\n",
      "step: 1321 loss: 4.863548278808594\n",
      "step: 1322 loss: 5.124317646026611\n",
      "step: 1323 loss: 4.990572929382324\n",
      "step: 1324 loss: 4.90562105178833\n",
      "step: 1325 loss: 5.0877366065979\n",
      "step: 1326 loss: 5.204028129577637\n",
      "step: 1327 loss: 5.013330459594727\n",
      "step: 1328 loss: 5.034438610076904\n",
      "step: 1329 loss: 4.89940881729126\n",
      "step: 1330 loss: 5.041737079620361\n",
      "step: 1331 loss: 4.848406791687012\n",
      "step: 1332 loss: 4.887752532958984\n",
      "step: 1333 loss: 5.184294700622559\n",
      "step: 1334 loss: 4.809980869293213\n",
      "step: 1335 loss: 4.984065532684326\n",
      "step: 1336 loss: 4.929998874664307\n",
      "step: 1337 loss: 4.96405029296875\n",
      "step: 1338 loss: 4.851832389831543\n",
      "step: 1339 loss: 5.144217491149902\n",
      "step: 1340 loss: 5.131631851196289\n",
      "step: 1341 loss: 4.974287509918213\n",
      "step: 1342 loss: 4.780093193054199\n",
      "step: 1343 loss: 5.004880905151367\n",
      "step: 1344 loss: 4.98242712020874\n",
      "step: 1345 loss: 4.988962173461914\n",
      "step: 1346 loss: 4.9663615226745605\n",
      "step: 1347 loss: 4.762599468231201\n",
      "step: 1348 loss: 5.322376728057861\n",
      "step: 1349 loss: 4.880331039428711\n",
      "step: 1350 loss: 4.875136852264404\n",
      "step: 1351 loss: 5.000844955444336\n",
      "step: 1352 loss: 4.698458194732666\n",
      "step: 1353 loss: 4.888733386993408\n",
      "step: 1354 loss: 5.051870346069336\n",
      "step: 1355 loss: 4.905062198638916\n",
      "step: 1356 loss: 4.7233967781066895\n",
      "step: 1357 loss: 4.916589736938477\n",
      "step: 1358 loss: 4.819183826446533\n",
      "step: 1359 loss: 5.233718395233154\n",
      "step: 1360 loss: 5.3394670486450195\n",
      "step: 1361 loss: 4.968054294586182\n",
      "step: 1362 loss: 5.026817321777344\n",
      "step: 1363 loss: 4.730167865753174\n",
      "step: 1364 loss: 4.8774261474609375\n",
      "step: 1365 loss: 4.84630012512207\n",
      "step: 1366 loss: 4.976655006408691\n",
      "step: 1367 loss: 4.758999347686768\n",
      "step: 1368 loss: 5.014291763305664\n",
      "step: 1369 loss: 5.01926851272583\n",
      "step: 1370 loss: 4.830369472503662\n",
      "step: 1371 loss: 4.681574821472168\n",
      "step: 1372 loss: 4.995815277099609\n",
      "step: 1373 loss: 5.061174392700195\n",
      "step: 1374 loss: 4.889557361602783\n",
      "step: 1375 loss: 4.81973123550415\n",
      "step: 1376 loss: 4.795335292816162\n",
      "step: 1377 loss: 4.897511005401611\n",
      "step: 1378 loss: 4.918034553527832\n",
      "step: 1379 loss: 4.770806789398193\n",
      "step: 1380 loss: 4.738064289093018\n",
      "step: 1381 loss: 4.738280296325684\n",
      "step: 1382 loss: 4.949781894683838\n",
      "step: 1383 loss: 4.774470329284668\n",
      "step: 1384 loss: 4.8663859367370605\n",
      "step: 1385 loss: 4.8628692626953125\n",
      "step: 1386 loss: 5.1910295486450195\n",
      "step: 1387 loss: 4.82604455947876\n",
      "step: 1388 loss: 4.810730457305908\n",
      "step: 1389 loss: 4.869345188140869\n",
      "step: 1390 loss: 5.098147869110107\n",
      "step: 1391 loss: 5.154608726501465\n",
      "step: 1392 loss: 4.956975936889648\n",
      "step: 1393 loss: 4.821903228759766\n",
      "step: 1394 loss: 4.833392143249512\n",
      "step: 1395 loss: 4.649620056152344\n",
      "step: 1396 loss: 4.623538017272949\n",
      "step: 1397 loss: 4.861325263977051\n",
      "step: 1398 loss: 5.0196943283081055\n",
      "step: 1399 loss: 5.218801975250244\n",
      "step: 1400 loss: 4.867515563964844\n",
      "step: 1401 loss: 4.756355285644531\n",
      "step: 1402 loss: 4.881547451019287\n",
      "step: 1403 loss: 5.0800299644470215\n",
      "step: 1404 loss: 4.621310234069824\n",
      "step: 1405 loss: 4.786884307861328\n",
      "step: 1406 loss: 4.796400547027588\n",
      "step: 1407 loss: 4.741830825805664\n",
      "step: 1408 loss: 4.989675998687744\n",
      "step: 1409 loss: 5.156233787536621\n",
      "step: 1410 loss: 4.8460798263549805\n",
      "step: 1411 loss: 4.8381123542785645\n",
      "step: 1412 loss: 4.803534507751465\n",
      "step: 1413 loss: 5.022934913635254\n",
      "step: 1414 loss: 4.7332940101623535\n",
      "step: 1415 loss: 5.097073078155518\n",
      "step: 1416 loss: 4.773462295532227\n",
      "step: 1417 loss: 4.863275051116943\n",
      "step: 1418 loss: 4.933134078979492\n",
      "step: 1419 loss: 5.136971950531006\n",
      "step: 1420 loss: 4.922108173370361\n",
      "step: 1421 loss: 5.2090959548950195\n",
      "step: 1422 loss: 5.036701679229736\n",
      "step: 1423 loss: 4.984204292297363\n",
      "step: 1424 loss: 5.026410102844238\n",
      "step: 1425 loss: 4.988975524902344\n",
      "step: 1426 loss: 4.765159606933594\n",
      "step: 1427 loss: 5.00172233581543\n",
      "step: 1428 loss: 4.920992851257324\n",
      "step: 1429 loss: 4.9247355461120605\n",
      "step: 1430 loss: 4.954277515411377\n",
      "step: 1431 loss: 4.600068092346191\n",
      "step: 1432 loss: 4.979187488555908\n",
      "step: 1433 loss: 4.814606666564941\n",
      "step: 1434 loss: 4.642358303070068\n",
      "step: 1435 loss: 4.726050853729248\n",
      "step: 1436 loss: 4.8465895652771\n",
      "step: 1437 loss: 4.822690963745117\n",
      "step: 1438 loss: 5.00743293762207\n",
      "step: 1439 loss: 4.816992282867432\n",
      "step: 1440 loss: 4.835505962371826\n",
      "step: 1441 loss: 4.856803894042969\n",
      "step: 1442 loss: 4.777778148651123\n",
      "step: 1443 loss: 4.733366012573242\n",
      "step: 1444 loss: 4.916590690612793\n",
      "step: 1445 loss: 4.769429683685303\n",
      "step: 1446 loss: 4.829638481140137\n",
      "step: 1447 loss: 5.188930988311768\n",
      "step: 1448 loss: 4.884261608123779\n",
      "step: 1449 loss: 4.812610149383545\n",
      "step: 1450 loss: 5.042181491851807\n",
      "step: 1451 loss: 4.950039386749268\n",
      "step: 1452 loss: 4.803494930267334\n",
      "step: 1453 loss: 4.830700874328613\n",
      "step: 1454 loss: 4.781251430511475\n",
      "step: 1455 loss: 4.833458423614502\n",
      "step: 1456 loss: 5.039238929748535\n",
      "step: 1457 loss: 4.695587158203125\n",
      "step: 1458 loss: 4.967565536499023\n",
      "step: 1459 loss: 4.985042572021484\n",
      "step: 1460 loss: 5.175702095031738\n",
      "step: 1461 loss: 4.788315296173096\n",
      "step: 1462 loss: 4.758295059204102\n",
      "step: 1463 loss: 4.833874702453613\n",
      "step: 1464 loss: 4.698143005371094\n",
      "step: 1465 loss: 4.763844013214111\n",
      "step: 1466 loss: 4.7599945068359375\n",
      "step: 1467 loss: 4.560315132141113\n",
      "step: 1468 loss: 5.100232124328613\n",
      "step: 1469 loss: 4.916644096374512\n",
      "step: 1470 loss: 4.886640548706055\n",
      "step: 1471 loss: 4.763663291931152\n",
      "step: 1472 loss: 5.092825412750244\n",
      "step: 1473 loss: 4.807014465332031\n",
      "step: 1474 loss: 4.7596940994262695\n",
      "step: 1475 loss: 4.904024600982666\n",
      "step: 1476 loss: 4.643467903137207\n",
      "step: 1477 loss: 4.72029972076416\n",
      "step: 1478 loss: 4.845551490783691\n",
      "step: 1479 loss: 4.830414772033691\n",
      "step: 1480 loss: 4.805662155151367\n",
      "step: 1481 loss: 4.774199485778809\n",
      "step: 1482 loss: 4.833065032958984\n",
      "step: 1483 loss: 4.814773082733154\n",
      "step: 1484 loss: 4.9874420166015625\n",
      "step: 1485 loss: 4.815064430236816\n",
      "step: 1486 loss: 4.834238052368164\n",
      "step: 1487 loss: 4.779631614685059\n",
      "step: 1488 loss: 4.79461669921875\n",
      "step: 1489 loss: 4.947608947753906\n",
      "step: 1490 loss: 4.798879623413086\n",
      "step: 1491 loss: 4.894651412963867\n",
      "step: 1492 loss: 4.866291046142578\n",
      "step: 1493 loss: 5.140763282775879\n",
      "step: 1494 loss: 5.024082660675049\n",
      "step: 1495 loss: 5.008232116699219\n",
      "step: 1496 loss: 4.841249942779541\n",
      "step: 1497 loss: 4.808582305908203\n",
      "step: 1498 loss: 4.892274856567383\n",
      "step: 1499 loss: 5.0871734619140625\n",
      "step: 1500 loss: 5.215888023376465\n",
      "step: 1501 loss: 4.626652717590332\n",
      "step: 1502 loss: 4.933178901672363\n",
      "step: 1503 loss: 4.683089256286621\n",
      "step: 1504 loss: 4.886314868927002\n",
      "step: 1505 loss: 4.819968223571777\n",
      "step: 1506 loss: 4.808388710021973\n",
      "step: 1507 loss: 4.790431976318359\n",
      "step: 1508 loss: 4.761548042297363\n",
      "step: 1509 loss: 4.814616680145264\n",
      "step: 1510 loss: 4.851330280303955\n",
      "step: 1511 loss: 5.029327392578125\n",
      "step: 1512 loss: 4.828969478607178\n",
      "step: 1513 loss: 4.527877330780029\n",
      "step: 1514 loss: 4.9739909172058105\n",
      "step: 1515 loss: 5.225656509399414\n",
      "step: 1516 loss: 4.809422016143799\n",
      "step: 1517 loss: 4.755163669586182\n",
      "step: 1518 loss: 4.704235553741455\n",
      "step: 1519 loss: 4.6857709884643555\n",
      "step: 1520 loss: 4.917217254638672\n",
      "step: 1521 loss: 4.619078636169434\n",
      "step: 1522 loss: 4.533137798309326\n",
      "step: 1523 loss: 4.587406635284424\n",
      "step: 1524 loss: 4.625241756439209\n",
      "step: 1525 loss: 4.8008928298950195\n",
      "step: 1526 loss: 4.95796537399292\n",
      "step: 1527 loss: 4.885438442230225\n",
      "step: 1528 loss: 4.529489040374756\n",
      "step: 1529 loss: 4.680633544921875\n",
      "step: 1530 loss: 4.813141345977783\n",
      "step: 1531 loss: 4.825793266296387\n",
      "step: 1532 loss: 5.097604274749756\n",
      "step: 1533 loss: 4.985256195068359\n",
      "step: 1534 loss: 4.772976875305176\n",
      "step: 1535 loss: 4.7931647300720215\n",
      "step: 1536 loss: 4.982661724090576\n",
      "step: 1537 loss: 4.753022193908691\n",
      "step: 1538 loss: 4.960080146789551\n",
      "step: 1539 loss: 4.8308515548706055\n",
      "step: 1540 loss: 4.9171271324157715\n",
      "step: 1541 loss: 4.8683390617370605\n",
      "step: 1542 loss: 4.77907133102417\n",
      "step: 1543 loss: 4.784616947174072\n",
      "step: 1544 loss: 4.866262912750244\n",
      "step: 1545 loss: 4.990572929382324\n",
      "step: 1546 loss: 4.772830963134766\n",
      "step: 1547 loss: 4.853535175323486\n",
      "step: 1548 loss: 4.8307881355285645\n",
      "step: 1549 loss: 4.8002543449401855\n",
      "step: 1550 loss: 4.949596881866455\n",
      "step: 1551 loss: 4.860314846038818\n",
      "step: 1552 loss: 4.701200008392334\n",
      "step: 1553 loss: 4.674749851226807\n",
      "step: 1554 loss: 5.035204887390137\n",
      "step: 1555 loss: 4.852369785308838\n",
      "step: 1556 loss: 4.65833044052124\n",
      "step: 1557 loss: 4.8629350662231445\n",
      "step: 1558 loss: 4.581383228302002\n",
      "step: 1559 loss: 4.774302005767822\n",
      "step: 1560 loss: 4.68627405166626\n",
      "step: 1561 loss: 4.446293830871582\n",
      "step: 1562 loss: 4.508296489715576\n",
      "step: 1563 loss: 4.906935214996338\n",
      "step: 1564 loss: 4.825857639312744\n",
      "step: 1565 loss: 4.510619163513184\n",
      "step: 1566 loss: 4.542878150939941\n",
      "step: 1567 loss: 4.8120503425598145\n",
      "step: 1568 loss: 4.701117992401123\n",
      "step: 1569 loss: 5.035556316375732\n",
      "step: 1570 loss: 4.534129619598389\n",
      "step: 1571 loss: 5.087144374847412\n",
      "step: 1572 loss: 5.067030429840088\n",
      "step: 1573 loss: 4.835617542266846\n",
      "step: 1574 loss: 4.701437950134277\n",
      "step: 1575 loss: 5.1366496086120605\n",
      "step: 1576 loss: 5.008164405822754\n",
      "step: 1577 loss: 4.8294243812561035\n",
      "step: 1578 loss: 4.852201461791992\n",
      "step: 1579 loss: 4.838143825531006\n",
      "step: 1580 loss: 4.642185211181641\n",
      "step: 1581 loss: 4.864912509918213\n",
      "step: 1582 loss: 5.13242769241333\n",
      "step: 1583 loss: 5.104958534240723\n",
      "step: 1584 loss: 4.8505048751831055\n",
      "step: 1585 loss: 4.964968204498291\n",
      "step: 1586 loss: 4.996183395385742\n",
      "step: 1587 loss: 4.840165615081787\n",
      "step: 1588 loss: 5.083526611328125\n",
      "step: 1589 loss: 5.213797569274902\n",
      "step: 1590 loss: 5.0790486335754395\n",
      "step: 1591 loss: 4.843392372131348\n",
      "step: 1592 loss: 4.602138519287109\n",
      "step: 1593 loss: 4.810475826263428\n",
      "step: 1594 loss: 4.8732404708862305\n",
      "step: 1595 loss: 4.924291133880615\n",
      "step: 1596 loss: 4.8250346183776855\n",
      "step: 1597 loss: 4.652411937713623\n",
      "step: 1598 loss: 4.551512718200684\n",
      "step: 1599 loss: 4.557706356048584\n",
      "step: 1600 loss: 4.790306091308594\n",
      "step: 1601 loss: 4.935221195220947\n",
      "step: 1602 loss: 4.7880778312683105\n",
      "step: 1603 loss: 5.08143949508667\n",
      "step: 1604 loss: 4.920454978942871\n",
      "step: 1605 loss: 4.795915126800537\n",
      "step: 1606 loss: 4.6181254386901855\n",
      "step: 1607 loss: 4.682379245758057\n",
      "step: 1608 loss: 4.847906112670898\n",
      "step: 1609 loss: 4.995409965515137\n",
      "step: 1610 loss: 4.615828514099121\n",
      "step: 1611 loss: 4.920713901519775\n",
      "step: 1612 loss: 4.685939311981201\n",
      "step: 1613 loss: 4.8708295822143555\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[902], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m             optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep: \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m}\u001b[39;00m\u001b[39m loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m train(model, train_data, \u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[902], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data, n_epochs)\u001b[0m\n\u001b[0;32m     14\u001b[0m h, c \u001b[39m=\u001b[39m repackage(h), repackage(c)\n\u001b[0;32m     15\u001b[0m outputs, (h, c) \u001b[39m=\u001b[39m model(inputs, h, c)\n\u001b[1;32m---> 17\u001b[0m targets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(y\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mint64))\u001b[39m.\u001b[39;49mtranspose(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mcontiguous()\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     18\u001b[0m tt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(targets\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mbatch_size \u001b[39m*\u001b[39m model\u001b[39m.\u001b[39mseq_len))\n\u001b[0;32m     20\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mvocab_size), tt)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, data, n_epochs=5):\n",
    "    \"\"\"\n",
    "    One epoch of training/validation (depending on flag is_train).\n",
    "    \"\"\"\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"####################### EPOCH {epoch} #########################\")\n",
    "        h, c = model.init_hidden_and_cell()\n",
    "        h, c = h.to(device), c.to(device)\n",
    "\n",
    "        # LOOP THROUGH MINIBATCHES\n",
    "        for step, (x, y) in enumerate(ptb_iterator(data, model.batch_size, model.seq_len)):\n",
    "            inputs = torch.from_numpy(x.astype(np.int64)).transpose(0, 1).contiguous().to(device)\n",
    "            model.zero_grad()\n",
    "            h, c = repackage(h), repackage(c)\n",
    "            outputs, (h, c) = model(inputs, h, c)\n",
    "\n",
    "            targets = torch.from_numpy(y.astype(np.int64)).transpose(0, 1).contiguous().to(device)\n",
    "            tt = torch.squeeze(targets.view(-1, model.batch_size * model.seq_len))\n",
    "\n",
    "            loss = loss_fn(outputs.contiguous().view(-1, model.vocab_size), tt)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"step: {step} loss: {loss.item()}\")\n",
    "\n",
    "train(model, train_data, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
